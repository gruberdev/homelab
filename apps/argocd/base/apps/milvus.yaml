apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: milvus
spec:
  project: apps
  source:
    repoURL: https://github.com/milvus-io/milvus-helm.git
    targetRevision: milvus-4.0.12
    path: charts/milvus
    helm:
      releaseName: milvus
      values: |
        cluster:
          enabled: false
        image:
          all:
            repository: milvusdb/milvus
            tag: v2.2.4
            pullPolicy: IfNotPresent
          tools:
            repository: milvusdb/milvus-config-tool
            tag: v0.1.1
            pullPolicy: IfNotPresent
        nodeSelector: {}
        tolerations: []
        affinity: {}
        labels: {}
        annotations: {}
        # Extra configs for milvus.yaml
        # If set, this config will merge into milvus.yaml
        # Please follow the config structure in the milvus.yaml
        # at https://github.com/milvus-io/milvus/blob/master/configs/milvus.yaml
        # Note: this config will be the top priority which will override the config
        # in the image and helm chart.
        extraConfigFiles:
          user.yaml: |+
            #    For example enable rest http for milvus proxy
            #    proxy:
            #      http:
            #        enabled: true
        ## Expose the Milvus service to be accessed from outside the cluster (LoadBalancer service).
        ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
        ## ref: http://kubernetes.io/docs/user-guide/services/
        ##
        service:
          type: ClusterIP
          port: 19530
          nodePort: ""
          annotations: {}
          labels: {}

          ## List of IP addresses at which the Milvus service is available
          ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
          ##
          externalIPs: []
          #   - externalIp1

          # LoadBalancerSourcesRange is a list of allowed CIDR values, which are combined with ServicePort to
          # set allowed inbound rules on the security group assigned to the master load balancer
          loadBalancerSourceRanges:
          - 0.0.0.0/0
          # Optionally assign a known public LB IP
          # loadBalancerIP: 1.2.3.4
        ingress:
          enabled: false
        serviceAccount:
          create: false
          name:
          annotations:
          labels:
        metrics:
          enabled: true
          serviceMonitor:
            enabled: false
            interval: "30s"
            scrapeTimeout: "10s"
            # Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
            additionalLabels: {}
        livenessProbe:
          enabled: true
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          enabled: true
          initialDelaySeconds: 90
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        log:
          level: "info"
          file:
            maxSize: 300
            maxAge: 10
            maxBackups: 20
          format: "text"
          persistence:
            mountPath: "/milvus/logs"
            enabled: false
            annotations:
              helm.sh/resource-policy: keep
            persistentVolumeClaim:
              existingClaim: "milvus-storage"
        heaptrack:
          image:
            repository: milvusdb/heaptrack
            tag: v0.1.0
            pullPolicy: IfNotPresent
        standalone:
          replicas: 1
          resources:
            limits:
              memory: 1Gi
              cpu: 550m
              ephemeral-storage: 30Gi
            requests:
              memory: 512Mi
              cpu: 150m
          nodeSelector: {}
          affinity: {}
          tolerations: []
          extraEnv: []
          heaptrack:
            enabled: false
          disk:
            enabled: true
            size:
              enabled: true  # Enable local storage size limit
          profiling:
            enabled: false  # Enable live profiling
          ## Default message queue for milvus standalone
          ## Supported value: rocksmq, pulsar and kafka
          messageQueue: rocksmq
          persistence:
            mountPath: "/var/lib/milvus"
            enabled: true
            annotations:
              helm.sh/resource-policy: keep
            persistentVolumeClaim:
              existingClaim: "milvus-storage"
        proxy:
          enabled: true
          replicas: 1
          resources:
            limits:
              memory: 1Gi
              cpu: 550m
            requests:
              memory: 512Mi
              cpu: 150m
          nodeSelector: {}
          affinity: {}
          tolerations: []
          extraEnv: []
          heaptrack:
            enabled: false
          profiling:
            enabled: false  # Enable live profiling
          http:
            enabled: true  # whether to enable http rest server
            debugMode:
              enabled: false
        rootCoordinator:
          enabled: false
        queryCoordinator:
          enabled: false
        queryNode:
          enabled: true
          replicas: 1
          resources:
            limits:
              memory: 1Gi
              cpu: 550m
              ephemeral-storage: 50Gi
            requests:
              memory: 512Mi
              cpu: 150m
          nodeSelector: {}
          affinity: {}
          tolerations: []
          extraEnv: []
          heaptrack:
            enabled: false
          disk:
            enabled: true  # Enable querynode load disk index, and search on disk index
            size:
              enabled: false  # Enable local storage size limit
          profiling:
            enabled: false  # Enable live profiling
        indexCoordinator:
          enabled: false
        indexNode:
          enabled: true
          replicas: 1
          resources:
            limits:
              memory: 1Gi
              cpu: 550m
              ephemeral-storage: 40Gi
            requests:
              memory: 512Mi
              cpu: 150m
          nodeSelector: {}
          affinity: {}
          tolerations: []
          extraEnv: []
          heaptrack:
            enabled: false
          profiling:
            enabled: false  # Enable live profiling
          disk:
            enabled: true  # Enable index node build disk vector index
            size:
              enabled: true  # Enable local storage size limit
        dataCoordinator:
          enabled: false
        dataNode:
          enabled: true
          replicas: 1
          resources:
            limits:
              memory: 1.5Gi
              cpu: 550m
              ephemeral-storage: 50Gi
            requests:
              memory: 512Mi
              cpu: 350m
          nodeSelector: {}
          affinity: {}
          tolerations: []
          extraEnv: []
          heaptrack:
            enabled: false
          profiling:
            enabled: false  # Enable live profiling
          enabled: true
          replicas: 1
          resources:
            limits:
              memory: 2Gi
              cpu: 1
            requests:
              memory: 512Mi
              cpu: 150m
          nodeSelector: {}
          affinity: {}
          tolerations: []
          extraEnv: []
          heaptrack:
            enabled: false
          profiling:
            enabled: false  # Enable live profiling
          activeStandby:
            enabled: false  # Enable active-standby when you set multiple replicas for Mixture coordinator

          service:
            annotations: {}
            labels: {}
            clusterIP: ""

        attu:
          enabled: false
        minio:
          enabled: false
        etcd:
          enabled: true
          name: etcd
          replicaCount: 1
          pdb:
            create: false
          image:
            repository: "milvusdb/etcd"
            tag: "3.5.5-r2"
            pullPolicy: IfNotPresent

          service:
            type: ClusterIP
            port: 2379
            peerPort: 2380

          auth:
            rbac:
              enabled: false

          persistence:
            enabled: true
            storageClass:
            accessMode: ReadWriteOnce
            size: 10Gi

          ## Enable auto compaction
          ## compaction by every 1000 revision
          ##
          autoCompactionMode: revision
          autoCompactionRetention: "1000"

          ## Increase default quota to 4G
          ##
          extraEnvVars:
          - name: ETCD_QUOTA_BACKEND_BYTES
            value: "4294967296"
          - name: ETCD_HEARTBEAT_INTERVAL
            value: "500"
          - name: ETCD_ELECTION_TIMEOUT
            value: "2500"
        pulsar:
          enabled: false
          name: pulsar
          fullnameOverride: ""
          persistence: true
          maxMessageSize: 5242880  # 5 * 1024 * 1024 Bytes, Maximum size of each message in pulsar.
          rbac:
            enabled: false
            psp: false
            limit_to_namespace: true

          affinity:
            anti_affinity: false
        ## enableAntiAffinity: no
          components:
            zookeeper: true
            bookkeeper: true
            # bookkeeper - autorecovery
            autorecovery: true
            broker: true
            functions: false
            proxy: true
            toolset: false
            pulsar_manager: false
          monitoring:
            prometheus: false
            grafana: false
            node_exporter: false
            alert_manager: false
          images:
            broker:
              repository: apachepulsar/pulsar
              pullPolicy: IfNotPresent
              tag: 2.8.2
            autorecovery:
              repository: apachepulsar/pulsar
              tag: 2.8.2
              pullPolicy: IfNotPresent
            zookeeper:
              repository: apachepulsar/pulsar
              pullPolicy: IfNotPresent
              tag: 2.8.2
            bookie:
              repository: apachepulsar/pulsar
              pullPolicy: IfNotPresent
              tag: 2.8.2
            proxy:
              repository: apachepulsar/pulsar
              pullPolicy: IfNotPresent
              tag: 2.8.2
            pulsar_manager:
              repository: apachepulsar/pulsar-manager
              pullPolicy: IfNotPresent
              tag: v0.1.0

          zookeeper:
            resources:
              requests:
                memory: 1024Mi
                cpu: 0.3
            configData:
              PULSAR_MEM: >
                -Xms1024m
                -Xmx1024m
              PULSAR_GC: >
                -Dcom.sun.management.jmxremote
                -Djute.maxbuffer=10485760
                -XX:+ParallelRefProcEnabled
                -XX:+UnlockExperimentalVMOptions
                -XX:+DoEscapeAnalysis
                -XX:+DisableExplicitGC
                -XX:+PerfDisableSharedMem
                -Dzookeeper.forceSync=no
            pdb:
              usePolicy: false

          bookkeeper:
            replicaCount: 1
            volumes:
              journal:
                name: journal
                size: 10Gi
              ledgers:
                name: ledgers
                size: 20Gi
            resources:
              limits:
                memory: 1.5Gi
                cpu: 500m
              requests:
                memory: 512Mi
                cpu: 150m
            configData:
              PULSAR_MEM: >
                -Xms1024m
                -Xmx1024m
                -XX:MaxDirectMemorySize=512m
              PULSAR_GC: >
                -Dio.netty.leakDetectionLevel=disabled
                -Dio.netty.recycler.linkCapacity=1024
                -XX:+UseG1GC -XX:MaxGCPauseMillis=10
                -XX:+ParallelRefProcEnabled
                -XX:+UnlockExperimentalVMOptions
                -XX:+DoEscapeAnalysis
                -XX:ParallelGCThreads=32
                -XX:ConcGCThreads=32
                -XX:G1NewSizePercent=50
                -XX:+DisableExplicitGC
                -XX:-ResizePLAB
                -XX:+ExitOnOutOfMemoryError
                -XX:+PerfDisableSharedMem
                -XX:+PrintGCDetails
              nettyMaxFrameSizeBytes: "104867840"
            pdb:
              usePolicy: false

          broker:
            component: broker
            podMonitor:
              enabled: false
            replicaCount: 1
            resources:
              limits:
                memory: 1.5Gi
                cpu: 550m
              requests:
                memory: 512Mi
                cpu: 150m
            configData:
              PULSAR_MEM: >
                -Xms4096m
                -Xmx4096m
                -XX:MaxDirectMemorySize=8192m
              PULSAR_GC: >
                -Dio.netty.leakDetectionLevel=disabled
                -Dio.netty.recycler.linkCapacity=1024
                -XX:+ParallelRefProcEnabled
                -XX:+UnlockExperimentalVMOptions
                -XX:+DoEscapeAnalysis
                -XX:ParallelGCThreads=32
                -XX:ConcGCThreads=32
                -XX:G1NewSizePercent=50
                -XX:+DisableExplicitGC
                -XX:-ResizePLAB
                -XX:+ExitOnOutOfMemoryError
              maxMessageSize: "104857600"
              defaultRetentionTimeInMinutes: "10080"
              defaultRetentionSizeInMB: "-1"
              backlogQuotaDefaultLimitGB: "8"
              ttlDurationDefaultInSeconds: "259200"
              subscriptionExpirationTimeMinutes: "30"
              backlogQuotaDefaultRetentionPolicy: producer_exception
            pdb:
              usePolicy: false

          autorecovery:
            resources:
              limits:
                memory: 1Gi
                cpu: 600m
              requests:
                memory: 512Mi
                cpu: 150m
          proxy:
            replicaCount: 1
            podMonitor:
              enabled: false
          resources:
            limits:
              memory: 1Gi
              cpu: 550m
            requests:
              memory: 512Mi
              cpu: 150m
            service:
              type: ClusterIP
            ports:
              pulsar: 6650
            configData:
              PULSAR_MEM: >
                -Xms2048m -Xmx2048m
              PULSAR_GC: >
                -XX:MaxDirectMemorySize=2048m
              httpNumThreads: "100"
            pdb:
              usePolicy: false

          pulsar_manager:
            service:
              type: ClusterIP

          pulsar_metadata:
            component: pulsar-init
            image:
              # the image used for running `pulsar-cluster-initialize` job
              repository: apachepulsar/pulsar
              tag: 2.8.2
        kafka:
          enabled: false
          name: kafka
          replicaCount: 1
          image:
            repository: bitnami/kafka
            tag: 3.1.0-debian-10-r52
          ## Increase graceful termination for kafka graceful shutdown
          terminationGracePeriodSeconds: "90"
          pdb:
            create: false

          ## Enable startup probe to prevent pod restart during recovering
          startupProbe:
            enabled: true

          ## Kafka Java Heap size
          heapOpts: "-Xmx4096m -Xms4096m"
          maxMessageBytes: _10485760
          defaultReplicationFactor: 1
          offsetsTopicReplicationFactor: 1
          ## Only enable time based log retention
          logRetentionHours: 168
          logRetentionBytes: _-1
          extraEnvVars:
          - name: KAFKA_CFG_MAX_PARTITION_FETCH_BYTES
            value: "5242880"
          - name: KAFKA_CFG_MAX_REQUEST_SIZE
            value: "5242880"
          - name: KAFKA_CFG_REPLICA_FETCH_MAX_BYTES
            value: "10485760"
          - name: KAFKA_CFG_FETCH_MESSAGE_MAX_BYTES
            value: "5242880"
          - name: KAFKA_CFG_LOG_ROLL_HOURS
            value: "24"

          persistence:
            enabled: true
            storageClass:
            accessMode: ReadWriteOnce
            size: 50Gi

          metrics:
            ## Prometheus Kafka exporter: exposes complimentary metrics to JMX exporter
            kafka:
              enabled: false
              image:
                repository: bitnami/kafka-exporter
                tag: 1.4.2-debian-10-r182

            ## Prometheus JMX exporter: exposes the majority of Kafkas metrics
            jmx:
              enabled: false
              image:
                repository: bitnami/jmx-exporter
                tag: 0.16.1-debian-10-r245

            ## To enable serviceMonitor, you must enable either kafka exporter or jmx exporter.
            ## And you can enable them both
            serviceMonitor:
              enabled: false

          service:
            type: ClusterIP
            ports:
              client: 9092

          zookeeper:
            enabled: true
            replicaCount: 1

        ## Configuration values for the mysql dependency
        ## ref: https://artifacthub.io/packages/helm/bitnami/mysql
        ##
        ## MySQL used for meta store is testing internally

        mysql:
          enabled: false
          name: mysql
          image:
            repository: bitnami/mysql
            tag: 8.0.23-debian-10-r84

          architecture: replication
          auth:
            rootPassword: "ChangeMe"
            createDatabase: true
            database: "milvus_meta"

          maxOpenConns: 20
          maxIdleConns: 5
          primary:
            name: primary
            resources:
              limits:
                memory: 1Gi
                cpu: 500m
              requests:
                memory: 512Mi
                cpu: 150m
            persistence:
              enabled: true
              storageClass: ""
              accessModes:
                - ReadWriteOnce
              size: 10Gi
          secondary:
            name: secondary
            replicaCount: 1
            resources:
              limits: {}
              requests: {}
            persistence:
              enabled: true
              storageClass: ""
              accessModes:
                - ReadWriteOnce
              size: 100Gi
        externalS3:
          enabled: true
          host: "ewr1.vultrobjects.com"
          port: "443"
          accessKey: "<path:kv/data/velero#id>"
          secretKey: "<path:kv/data/velero#key>"
          useSSL: true
          bucketName: "milvus-wa"
          rootPath: ""
          useIAM: false
          cloudProvider: "aws"
  destination:
    namespace: milvus-system
    name: in-cluster
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: true
    syncOptions:
    - Validate=false
    - CreateNamespace=true
    - PrunePropagationPolicy=foreground
    - PruneLast=false
    - ApplyOutOfSyncOnly=false
    - Prune=true
    retry:
      limit: 3
      backoff:
        duration: 60s
        factor: 2
        maxDuration: 15m
