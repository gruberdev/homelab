apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus
spec:
  project: monitoring
  source:
    repoURL: https://github.com/prometheus-community/helm-charts.git
    targetRevision: main
    path: charts/kube-prometheus-stack
    helm:
      skipCrds: true
      releaseName: kube-prometheus
      values: |
        nameOverride: ""
        namespaceOverride: "monitoring"
        kubeTargetVersionOverride: ""
        kubeVersionOverride: ""
        fullnameOverride: ""
        commonLabels: {}
        defaultRules:
          create: true
          rules:
            etcd: false
            k8s: false
            kubeApiserverAvailability: false
            kubeApiserverBurnrate: false
            kubeApiserverHistogram: false
            kubeApiserverSlos: false
            kubeControllerManager: false
            kubelet: false
            kubeProxy: false
            kubernetesApps: false
            kubernetesResources: false
            kubernetesStorage: false
            kubernetesSystem: false
            kubeScheduler: false
            alertmanager: true
            configReloaders: true
            general: true
            kubePrometheusGeneral: true
            kubePrometheusNodeRecording: true
            kubernetesApps: true
            kubernetesResources: true
            kubernetesStorage: true
            kubernetesSystem: true
            kubeSchedulerAlerting: true
            kubeSchedulerRecording: true
            kubeStateMetrics: true
            network: true
            node: true
            nodeExporterAlerting: true
            nodeExporterRecording: true
            prometheus: true
            prometheusOperator: true
          appNamespacesTarget: ".*"
          labels: {}
          annotations: {}
          additionalRuleLabels: {}
          additionalRuleAnnotations: {}
          runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
          disabled: {}
        additionalPrometheusRulesMap: {}
        global:
          rbac:
            create: true
            createAggregateClusterRoles: false
            pspEnabled: false
            pspAnnotations: {}
          imagePullSecrets: []
        alertmanager:
          enabled: true
          annotations: {}
          apiVersion: v2
          serviceAccount:
            create: true
            name: ""
            annotations: {}
          podDisruptionBudget:
            enabled: false
            minAvailable: 1
            maxUnavailable: ""
          config:
            global:
              resolve_timeout: 5m
            inhibit_rules:
              - source_matchers:
                  - 'severity = critical'
                target_matchers:
                  - 'severity =~ warning|info'
                equal:
                  - 'namespace'
                  - 'alertname'
              - source_matchers:
                  - 'severity = warning'
                target_matchers:
                  - 'severity = info'
                equal:
                  - 'namespace'
                  - 'alertname'
              - source_matchers:
                  - 'alertname = InfoInhibitor'
                target_matchers:
                  - 'severity = info'
                equal:
                  - 'namespace'
            route:
              group_by: ['namespace']
              group_wait: 30s
              group_interval: 5m
              repeat_interval: 12h
              receiver: 'null'
              routes:
              - receiver: 'null'
                matchers:
                  - alertname =~ "InfoInhibitor|Watchdog"
            receivers:
            - name: 'null'
            templates:
            - '/etc/alertmanager/config/*.tmpl'
          tplConfig: false
          templateFiles: {}
          ingress:
            enabled: false
          secret:
            annotations: {}
          ingressPerReplica:
            enabled: false
          service:
            annotations: {}
            labels: {}
            clusterIP: ""

            ## Port for Alertmanager Service to listen on
            ##
            port: 9093
            ## To be used with a proxy extraContainer port
            ##
            targetPort: 9093
            ## Port to expose on each node
            ## Only used if service.type is 'NodePort'
            ##
            nodePort: 30903
            ## List of IP addresses at which the Prometheus server service is available
            ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
            ##

            ## Additional ports to open for Alertmanager service
            additionalPorts: []
            # additionalPorts:
            # - name: authenticated
            #   port: 8081
            #   targetPort: 8081

            externalIPs: []
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

          ## Configuration for creating a separate Service for each statefulset Alertmanager replica
          ##
          servicePerReplica:
            enabled: false
            annotations: {}

            ## Port for Alertmanager Service per replica to listen on
            ##
            port: 9093

            ## To be used with a proxy extraContainer port
            targetPort: 9093

            ## Port to expose on each node
            ## Only used if servicePerReplica.type is 'NodePort'
            ##
            nodePort: 30904

            ## Loadbalancer source IP ranges
            ## Only used if servicePerReplica.type is "LoadBalancer"
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

          ## If true, create a serviceMonitor for alertmanager
          ##
          serviceMonitor:
            ## Scrape interval. If not set, the Prometheus default scrape interval is used.
            ##
            interval: ""
            selfMonitor: true

            ## proxyUrl: URL of a proxy that should be used for scraping.
            ##
            proxyUrl: ""

            ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
            scheme: ""

            ## enableHttp2: Whether to enable HTTP2.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
            enableHttp2: false

            ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
            ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
            tlsConfig: {}

            bearerTokenFile:

            ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            metricRelabelings: []
            # - action: keep
            #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
            #   sourceLabels: [__name__]

            ## RelabelConfigs to apply to samples before scraping
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            relabelings: []
            # - sourceLabels: [__meta_kubernetes_pod_node_name]
            #   separator: ;
            #   regex: ^(.*)$
            #   targetLabel: nodename
            #   replacement: $1
            #   action: replace

          ## Settings affecting alertmanagerSpec
          ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
          ##
          alertmanagerSpec:
            ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
            ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
            ##
            podMetadata: {}

            ## Image of Alertmanager
            ##
            image:
              repository: quay.io/prometheus/alertmanager
              tag: v0.24.0
              sha: ""

            ## If true then the user will be responsible to provide a secret with alertmanager configuration
            ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
            ##
            useExistingSecret: false

            ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
            ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
            ##
            secrets: []

            ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
            ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
            ##
            configMaps: []

            ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
            ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
            ##
            # configSecret:

            ## WebTLSConfig defines the TLS parameters for HTTPS
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec
            web: {}

            ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
            ##
            alertmanagerConfigSelector: {}
            ## Example which selects all alertmanagerConfig resources
            ## with label "alertconfig" with values any of "example-config" or "example-config-2"
            # alertmanagerConfigSelector:
            #   matchExpressions:
            #     - key: alertconfig
            #       operator: In
            #       values:
            #         - example-config
            #         - example-config-2
            #
            ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
            # alertmanagerConfigSelector:
            #   matchLabels:
            #     role: example-config

            ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
            ##
            alertmanagerConfigNamespaceSelector: {}
            alertmanagerConfiguration: {}
            logFormat: logfmt
            logLevel: info
            replicas: 1
            retention: 120h
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi

            ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
            ##
            externalUrl:

            ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
            ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
            ##
            routePrefix: /

            ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
            ##
            paused: false

            ## Define which Nodes the Pods are scheduled on.
            ## ref: https://kubernetes.io/docs/user-guide/node-selection/
            ##
            nodeSelector:
              kubernetes.io/hostname: node-two
            ## Define resources requests and limits for single Pods.
            ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
            ##
            resources:
              limits:
                cpu: 500m
                memory: 1Gi
              requests:
                cpu: 300m
                memory: 512Mi

            ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
            ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
            ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
            ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
            ##
            podAntiAffinity: ""

            ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
            ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
            ##
            podAntiAffinityTopologyKey: kubernetes.io/hostname

            ## Assign custom affinity rules to the alertmanager instance
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
            ##
            affinity: {}
            # nodeAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     nodeSelectorTerms:
            #     - matchExpressions:
            #       - key: kubernetes.io/e2e-az-name
            #         operator: In
            #         values:
            #         - e2e-az1
            #         - e2e-az2

            ## If specified, the pod's tolerations.
            ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
            ##
            tolerations: []
            # - key: "key"
            #   operator: "Equal"
            #   value: "value"
            #   effect: "NoSchedule"

            ## If specified, the pod's topology spread constraints.
            ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
            ##
            topologySpreadConstraints: []
            # - maxSkew: 1
            #   topologyKey: topology.kubernetes.io/zone
            #   whenUnsatisfiable: DoNotSchedule
            #   labelSelector:
            #     matchLabels:
            #       app: alertmanager

            ## SecurityContext holds pod-level security attributes and common container settings.
            ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
            ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
            ##
            securityContext:
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
              fsGroup: 2000

            ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
            ## Note this is only for the Alertmanager UI, not the gossip communication.
            ##
            listenLocal: false

            ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
            ##
            containers: []

            # Additional volumes on the output StatefulSet definition.
            volumes: []

            # Additional VolumeMounts on the output StatefulSet definition.
            volumeMounts: []

            ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
            ## (permissions, dir tree) on mounted volumes before starting prometheus
            initContainers: []

            ## Priority class assigned to the Pods
            ##
            priorityClassName: ""

            ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
            ##
            additionalPeers: []

            ## PortName to use for Alert Manager.
            ##
            portName: "http-web"

            ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
            ##
            clusterAdvertiseAddress: false

            ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
            ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
            forceEnableClusterMode: false

            ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
            ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
            minReadySeconds: 0

          ## ExtraSecret can be used to store various data in an extra secret
          ## (use it for example to store hashed basic auth credentials)
          extraSecret:
            ## if not set, name will be auto generated
            # name: ""
            annotations: {}
            data: {}
          #   auth: |
          #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
          #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.
        grafana:
          enabled: true
          namespaceOverride: monitoring
          plugins:
            - grafana-piechart-panel
            - grafana-clock-panel
          dashboardProviders:
            dashboardproviders.yaml:
              apiVersion: 1
              providers:
              - name: unifi
                orgId: 1
                folder: 'Unifi'
                type: file
                disableDeletion: true
                editable: true
                options:
                  path: /var/lib/grafana/dashboards/unifi
              - name: longhorn
                orgId: 1
                folder: 'Longhorn'
                type: file
                disableDeletion: true
                editable: true
                options:
                  path: /var/lib/grafana/dashboards/longhorn
          dashboards:
            unifi:
              unifiPollerClientDPI:
                gnetId: 11310
                revision: 4
                datasource: Prometheus
              unifiPollerClientInsights:
                gnetId: 11315
                revision: 8
                datasource: Prometheus
              unifiPollerNetworkSites:
                gnetId: 11311
                revision: 4
                datasource: Prometheus
              unifiPollerUAPInsights:
                gnetId: 11314
                revision: 8
                datasource: Prometheus
              unifiPollerUSGInsights:
                gnetId: 11313
                revision: 8
                datasource: Prometheus
              unifiPollerUSWInsights:
                gnetId: 11312
                revision: 8
                datasource: Prometheus
            longhorn:
              longhornMonitoring:
                gnetId: 11310
                revision: 4
                datasource: Prometheus
          forceDeployDatasources: false
          forceDeployDashboards: false
          defaultDashboardsEnabled: true
          defaultDashboardsTimezone: "America/Sao_Paulo"
          adminPassword: <path:secret/data/kube-prometheus#grafana-admin-password>
          rbac:
            pspEnabled: false
          ingress:
            enabled: false
          sidecar:
            dashboards:
              enabled: true
              labelValue: "1"
              folderAnnotation: grafana_folder
              searchNamespace: "ALL"
              multicluster:
                global:
                  enabled: false
                etcd:
                  enabled: false
              provider:
                foldersFromFilesStructure: true
            datasources:
              enabled: true
              defaultDatasourceEnabled: true
              uid: prometheus
              # url: http://prometheus-stack-prometheus:9090/
              annotations: {}
              createPrometheusReplicasDatasources: false
              label: grafana_datasource
              labelValue: "1"
              exemplarTraceIdDestinations: {}
          extraConfigmapMounts: []
          # - name: certs-configmap
          #   mountPath: /etc/grafana/ssl/
          #   configMap: certs-configmap
          #   readOnly: true
          deleteDatasources: []
          additionalDataSources: []
          service:
            portName: http-web
          serviceMonitor:
            enabled: true
            path: "/metrics"
            namespace: monitoring
            labels: {}
            interval: ""
            scheme: http
            tlsConfig: {}
            scrapeTimeout: 30s
            relabelings: []
        kubeApiServer:
          enabled: false
        kubelet:
          enabled: false
        kubeControllerManager:
          enabled: false
        coreDns:
          enabled: true
          service:
            port: 9153
            targetPort: 9153
          serviceMonitor:
            interval: ""
            proxyUrl: ""
            metricRelabelings: []
            relabelings: []
            additionalLabels: {}
        kubeDns:
          enabled: true
        kubeEtcd:
          enabled: false
        kubeScheduler:
          enabled: false
        kubeProxy:
          enabled: false
        kubeStateMetrics:
          enabled: true
        kube-state-metrics:
          namespaceOverride: monitoring
          rbac:
            create: true
          releaseLabel: true
          prometheus:
            monitor:
              enabled: true
              interval: ""
              scrapeTimeout: ""
              proxyUrl: ""
              honorLabels: true
              metricRelabelings: []
              relabelings: []
          selfMonitor:
            enabled: false
        nodeExporter:
          enabled: true
        prometheus-node-exporter:
          namespaceOverride: monitoring
          podLabels:
            jobLabel: node-exporter
          releaseLabel: true
          extraArgs:
            - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
            - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          service:
            portName: http-metrics
          prometheus:
            monitor:
              enabled: true
              jobLabel: jobLabel

              ## Scrape interval. If not set, the Prometheus default scrape interval is used.
              ##
              interval: ""

              ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
              ##
              scrapeTimeout: ""

              ## proxyUrl: URL of a proxy that should be used for scraping.
              ##
              proxyUrl: ""

              ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
              ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
              ##
              metricRelabelings: []
              # - sourceLabels: [__name__]
              #   separator: ;
              #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
              #   replacement: $1
              #   action: drop

              ## RelabelConfigs to apply to samples before scraping
              ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
              ##
              relabelings: []
              # - sourceLabels: [__meta_kubernetes_pod_node_name]
              #   separator: ;
              #   regex: ^(.*)$
              #   targetLabel: nodename
              #   replacement: $1
              #   action: replace
          rbac:
            ## If true, create PSPs for node-exporter
            ##
            pspEnabled: false

        ## Manages Prometheus and Alertmanager components
        ##
        prometheusOperator:
          enabled: true
          tls:
            enabled: false
            tlsMinVersion: VersionTLS13
            internalPort: 10250
          ## rules from making their way into prometheus and potentially preventing the container from starting
          admissionWebhooks:
            failurePolicy: Fail
            ## The default timeoutSeconds is 10 and the maximum value is 30.
            timeoutSeconds: 10
            enabled: false
            ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
            ## If unspecified, system trust roots on the apiserver are used.
            caBundle: ""
            ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
            ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
            ## certs ahead of time if you wish.
            ##
            annotations: {}
            #   argocd.argoproj.io/hook: PreSync
            #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
            patch:
              enabled: true
              image:
                repository: k8s.gcr.io/ingress-nginx/kube-webhook-certgen
                tag: v1.3.0
                sha: ""
                pullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 400m
                  memory: 512Mi
                requests:
                  cpu: 200m
                  memory: 256Mi
              ## Provide a priority class name to the webhook patching job
              ##
              priorityClassName: ""
              annotations: {}
              #   argocd.argoproj.io/hook: PreSync
              #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
              podAnnotations: {}
              nodeSelector:
                kubernetes.io/hostname: node-two
              affinity: {}
              tolerations: []
              securityContext:
                runAsGroup: 2000
                runAsNonRoot: true
                runAsUser: 2000

            # Security context for create job container
            createSecretJob:
              securityContext: {}

              # Security context for patch job container
            patchWebhookJob:
              securityContext: {}

            # Use certmanager to generate webhook certs
            certManager:
              enabled: false
          ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
          ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
          ##
          namespaces: {}
            # releaseNamespace: true
            # additional:
            # - kube-system

          ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
          ##
          denyNamespaces: []

          ## Filter namespaces to look for prometheus-operator custom resources
          ##
          alertmanagerInstanceNamespaces: []
          alertmanagerConfigNamespaces: []
          prometheusInstanceNamespaces: []
          thanosRulerInstanceNamespaces: []

          ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
          ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
          ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
          ##
          # clusterDomain: "cluster.local"

          networkPolicy:
            ## Enable creation of NetworkPolicy resources.
            ##
            enabled: false

          ## Service account for Alertmanager to use.
          ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
          ##
          serviceAccount:
            create: true
            name: ""

          ## Configuration for Prometheus operator service
          ##
          service:
            annotations: {}
            labels: {}
            clusterIP: ""

          ## Port to expose on each node
          ## Only used if service.type is 'NodePort'
          ##
            nodePort: 30080

            nodePortTls: 30443

          ## Additional ports to open for Prometheus service
          ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
          ##
            additionalPorts: []

          ## Loadbalancer IP
          ## Only use if service.type is "LoadBalancer"
          ##
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

          ## Service type
          ## NodePort, ClusterIP, LoadBalancer
          ##
            type: ClusterIP

            ## List of IP addresses at which the Prometheus server service is available
            ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
            ##
            externalIPs: []

          # ## Labels to add to the operator deployment
          # ##
          labels: {}

          ## Annotations to add to the operator deployment
          ##
          annotations: {}

          ## Labels to add to the operator pod
          ##
          podLabels: {}

          ## Annotations to add to the operator pod
          ##
          podAnnotations: {}

          ## Assign a PriorityClassName to pods if set
          # priorityClassName: ""

          ## Define Log Format
          # Use logfmt (default) or json logging
          # logFormat: logfmt

          ## Decrease log verbosity to errors only
          # logLevel: error

          ## If true, the operator will create and maintain a service for scraping kubelets
          ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
          ##
          kubeletService:
            enabled: false
          serviceMonitor:
            relabelings:
            - sourceLabels:
              - __address__
              action: replace
              targetLabel: job
              replacement: prometheus-operator
            interval: ""
            scrapeTimeout: ""
            selfMonitor: true
            metricRelabelings: []
          resources:
            limits:
              cpu: 350m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          hostNetwork: false
          nodeSelector:
            kubernetes.io/hostname: node-two
          tolerations: []
          # - key: "key"
          #   operator: "Equal"
          #   value: "value"
          #   effect: "NoSchedule"

          ## Assign custom affinity rules to the prometheus operator
          ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
          ##
          affinity: {}
            # nodeAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     nodeSelectorTerms:
            #     - matchExpressions:
            #       - key: kubernetes.io/e2e-az-name
            #         operator: In
            #         values:
            #         - e2e-az1
            #         - e2e-az2
          dnsConfig: {}
            # nameservers:
            #   - 1.2.3.4
            # searches:
            #   - ns1.svc.cluster-domain.example
            #   - my.dns.search.suffix
            # options:
            #   - name: ndots
            #     value: "2"
          #   - name: edns0
          securityContext:
            fsGroup: 65534
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534

          ## Container-specific security context configuration
          ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
          ##
          containerSecurityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true

          # Enable vertical pod autoscaler support for prometheus-operator
          verticalPodAutoscaler:
            enabled: false
            # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
            controlledResources: []

            # Define the max allowed resources for the pod
            maxAllowed: {}
            # cpu: 200m
            # memory: 100Mi
            # Define the min allowed resources for the pod
            minAllowed: {}
            # cpu: 200m
            # memory: 100Mi

            updatePolicy:
              # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
              # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
              updateMode: Auto

          ## Prometheus-operator image
          ##
          image:
            repository: quay.io/prometheus-operator/prometheus-operator
            tag: v0.60.1
            sha: ""
            pullPolicy: IfNotPresent

          ## Prometheus image to use for prometheuses managed by the operator
          ##
          # prometheusDefaultBaseImage: quay.io/prometheus/prometheus

          ## Alertmanager image to use for alertmanagers managed by the operator
          ##
          # alertmanagerDefaultBaseImage: quay.io/prometheus/alertmanager

          ## Prometheus-config-reloader
          ##
          prometheusConfigReloader:
            image:
              repository: quay.io/prometheus-operator/prometheus-config-reloader
              tag: v0.60.1
              sha: ""

            # resource config for prometheusConfigReloader
            resources:
              requests:
                cpu: 250m
                memory: 256Mi
              limits:
                cpu: 450m
                memory: 512Mi

          ## Thanos side-car image when configured
          ##
          thanosImage:
            repository: quay.io/thanos/thanos
            tag: v0.28.1
            sha: ""

          ## Set a Field Selector to filter watched secrets
          ##
          secretFieldSelector: ""

        ## Deploy a Prometheus instance
        ##
        prometheus:
          enabled: true
          annotations: {}
          serviceAccount:
            create: true
            name: ""
            annotations: {}

          # Service for thanos service discovery on sidecar
          # Enable this can make Thanos Query can use
          # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
          # Thanos sidecar on prometheus nodes
          # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
          thanosService:
            enabled: false
            annotations: {}
            labels: {}

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

            ## gRPC port config
            portName: grpc
            port: 10901
            targetPort: "grpc"

            ## HTTP port config (for metrics)
            httpPortName: http
            httpPort: 10902
            targetHttpPort: "http"

            ## ClusterIP to assign
            # Default is to make this a headless service ("None")
            clusterIP: "None"

            ## Port to expose on each node, if service type is NodePort
            ##
            nodePort: 30901
            httpNodePort: 30902

          # ServiceMonitor to scrape Sidecar metrics
          # Needs thanosService to be enabled as well
          thanosServiceMonitor:
            enabled: false
            interval: ""

            ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
            scheme: ""

            ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
            ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
            tlsConfig: {}

            bearerTokenFile:

            ## Metric relabel configs to apply to samples before ingestion.
            metricRelabelings: []

            ## relabel configs to apply to samples before ingestion.
            relabelings: []

          # Service for external access to sidecar
          # Enabling this creates a service to expose thanos-sidecar outside the cluster.
          thanosServiceExternal:
            enabled: false
            annotations: {}
            labels: {}
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## gRPC port config
            portName: grpc
            port: 10901
            targetPort: "grpc"

            ## HTTP port config (for metrics)
            httpPortName: http
            httpPort: 10902
            targetHttpPort: "http"

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: LoadBalancer

            ## Port to expose on each node
            ##
            nodePort: 30901
            httpNodePort: 30902

          ## Configuration for Prometheus service
          ##
          service:
            annotations: {}
            labels: {}
            clusterIP: ""

            ## Port for Prometheus Service to listen on
            ##
            port: 9090

            ## To be used with a proxy extraContainer port
            targetPort: 9090

            ## List of IP addresses at which the Prometheus server service is available
            ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
            ##
            externalIPs: []

            ## Port to expose on each node
            ## Only used if service.type is 'NodePort'
            ##
            nodePort: 30090

            ## Loadbalancer IP
            ## Only use if service.type is "LoadBalancer"
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

            ## Additional port to define in the Service
            additionalPorts: []
            # additionalPorts:
            # - name: authenticated
            #   port: 8081
            #   targetPort: 8081

            ## Consider that all endpoints are considered "ready" even if the Pods themselves are not
            ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec
            publishNotReadyAddresses: false

            sessionAffinity: ""

          ## Configuration for creating a separate Service for each statefulset Prometheus replica
          ##
          servicePerReplica:
            enabled: false
            annotations: {}

            ## Port for Prometheus Service per replica to listen on
            ##
            port: 9090

            ## To be used with a proxy extraContainer port
            targetPort: 9090

            ## Port to expose on each node
            ## Only used if servicePerReplica.type is 'NodePort'
            ##
            nodePort: 30091

            ## Loadbalancer source IP ranges
            ## Only used if servicePerReplica.type is "LoadBalancer"
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

          ## Configure pod disruption budgets for Prometheus
          ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
          ## This configuration is immutable once created and will require the PDB to be deleted to be changed
          ## https://github.com/kubernetes/kubernetes/issues/45398
          ##
          podDisruptionBudget:
            enabled: false
            minAvailable: 1
            maxUnavailable: ""

          # Ingress exposes thanos sidecar outside the cluster
          thanosIngress:
            enabled: false

            # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
            # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
            # ingressClassName: nginx

            annotations: {}
            labels: {}
            servicePort: 10901

            ## Port to expose on each node
            ## Only used if service.type is 'NodePort'
            ##
            nodePort: 30901

            ## Hosts must be provided if Ingress is enabled.
            ##
            hosts: []
              # - thanos-gateway.domain.com

            ## Paths to use for ingress rules
            ##
            paths: []
            # - /

            ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
            ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
            # pathType: ImplementationSpecific

            ## TLS configuration for Thanos Ingress
            ## Secret must be manually created in the namespace
            ##
            tls: []
            # - secretName: thanos-gateway-tls
            #   hosts:
            #   - thanos-gateway.domain.com
            #

          ## ExtraSecret can be used to store various data in an extra secret
          ## (use it for example to store hashed basic auth credentials)
          extraSecret:
            ## if not set, name will be auto generated
            # name: ""
            annotations: {}
            data: {}
          #   auth: |
          #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
          #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

          ingress:
            enabled: false

            # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
            # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
            # ingressClassName: nginx

            annotations: {}
            labels: {}

            ## Redirect ingress to an additional defined port on the service
            # servicePort: 8081

            ## Hostnames.
            ## Must be provided if Ingress is enabled.
            ##
            # hosts:
            #   - prometheus.domain.com
            hosts: []

            ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
            ##
            paths: []
            # - /

            ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
            ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
            # pathType: ImplementationSpecific

            ## TLS configuration for Prometheus Ingress
            ## Secret must be manually created in the namespace
            ##
            tls: []
              # - secretName: prometheus-general-tls
              #   hosts:
              #     - prometheus.example.com

          ## Configuration for creating an Ingress that will map to each Prometheus replica service
          ## prometheus.servicePerReplica must be enabled
          ##
          ingressPerReplica:
            enabled: false

            # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
            # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
            # ingressClassName: nginx

            annotations: {}
            labels: {}

            ## Final form of the hostname for each per replica ingress is
            ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
            ##
            ## Prefix for the per replica ingress that will have `-$replicaNumber`
            ## appended to the end
            hostPrefix: ""
            ## Domain that will be used for the per replica ingress
            hostDomain: ""

            ## Paths to use for ingress rules
            ##
            paths: []
            # - /

            ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
            ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
            # pathType: ImplementationSpecific

            ## Secret name containing the TLS certificate for Prometheus per replica ingress
            ## Secret must be manually created in the namespace
            tlsSecretName: ""

            ## Separated secret for each per replica Ingress. Can be used together with cert-manager
            ##
            tlsSecretPerReplica:
              enabled: false
              ## Final form of the secret for each per replica ingress is
              ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
              ##
              prefix: "prometheus"

          ## Configure additional options for default pod security policy for Prometheus
          ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
          podSecurityPolicy:
            allowedCapabilities: []
            allowedHostPaths: []
            volumes: []

          serviceMonitor:
            ## Scrape interval. If not set, the Prometheus default scrape interval is used.
            ##
            interval: ""
            selfMonitor: true

            ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
            scheme: ""

            ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
            ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
            tlsConfig: {}

            bearerTokenFile:

            ## Metric relabel configs to apply to samples before ingestion.
            ##
            metricRelabelings: []
            # - action: keep
            #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
            #   sourceLabels: [__name__]

            #   relabel configs to apply to samples before ingestion.
            ##
            relabelings: []
            # - sourceLabels: [__meta_kubernetes_pod_node_name]
            #   separator: ;
            #   regex: ^(.*)$
            #   targetLabel: nodename
            #   replacement: $1
            #   action: replace

          ## Settings affecting prometheusSpec
          ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
          ##
          prometheusSpec:
            retention: 3d
            disableCompaction: false
            apiserverConfig: {}
            additionalArgs: []
            scrapeInterval: ""
            scrapeTimeout: ""
            evaluationInterval: ""
            listenLocal: false
            enableAdminAPI: false
            web: {}
            exemplars: ""
            enableFeatures: []
            image:
              repository: quay.io/prometheus/prometheus
              tag: v2.39.1
              sha: ""
            tolerations: []
            #  - key: "key"
            #    operator: "Equal"
            #    value: "value"
            #    effect: "NoSchedule"
            topologySpreadConstraints: []
            # - maxSkew: 1
            #   topologyKey: topology.kubernetes.io/zone
            #   whenUnsatisfiable: DoNotSchedule
            #   labelSelector:
            #     matchLabels:
            #       app: prometheus
            alertingEndpoints: []
            # - name: ""
            #   namespace: ""
            #   port: http
            #   scheme: http
            #   pathPrefix: ""
            #   tlsConfig: {}
            #   bearerTokenFile: ""
            #   apiVersion: v2

            ## External labels to add to any time series or alerts when communicating with external systems
            ##
            externalLabels: {}

            ## enable --web.enable-remote-write-receiver flag on prometheus-server
            ##
            enableRemoteWriteReceiver: false

            ## Name of the external label used to denote replica name
            ##
            replicaExternalLabelName: ""

            ## If true, the Operator won't add the external label used to denote replica name
            ##
            replicaExternalLabelNameClear: false

            ## Name of the external label used to denote Prometheus instance name
            ##
            prometheusExternalLabelName: ""

            ## If true, the Operator won't add the external label used to denote Prometheus instance name
            ##
            prometheusExternalLabelNameClear: false

            ## External URL at which Prometheus will be reachable.
            ##
            externalUrl: ""

            ## Define which Nodes the Pods are scheduled on.
            ## ref: https://kubernetes.io/docs/user-guide/node-selection/
            ##
            nodeSelector:
              kubernetes.io/hostname: node-two

            ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
            ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
            ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
            ## with the new list of secrets.
            ##
            secrets: []

            ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
            ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
            ##
            configMaps: []

            ## QuerySpec defines the query command line flags when starting Prometheus.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec
            ##
            query: {}

            ## Namespaces to be selected for PrometheusRules discovery.
            ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            ruleNamespaceSelector: {}

            ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the PrometheusRule resources created
            ##
            ruleSelectorNilUsesHelmValues: true

            ## PrometheusRules to be selected for target discovery.
            ## If {}, select all PrometheusRules
            ##
            ruleSelector: {}
            ## Example which select all PrometheusRules resources
            ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
            # ruleSelector:
            #   matchExpressions:
            #     - key: prometheus
            #       operator: In
            #       values:
            #         - example-rules
            #         - example-rules-2
            #
            ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
            # ruleSelector:
            #   matchLabels:
            #     role: example-rules

            ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the servicemonitors created
            ##
            serviceMonitorSelectorNilUsesHelmValues: true

            ## ServiceMonitors to be selected for target discovery.
            ## If {}, select all ServiceMonitors
            ##
            serviceMonitorSelector: {}
            ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
            # serviceMonitorSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## Namespaces to be selected for ServiceMonitor discovery.
            ##
            serviceMonitorNamespaceSelector: {}
            ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
            # serviceMonitorNamespaceSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the podmonitors created
            ##
            podMonitorSelectorNilUsesHelmValues: true

            ## PodMonitors to be selected for target discovery.
            ## If {}, select all PodMonitors
            ##
            podMonitorSelector: {}
            ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
            # podMonitorSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## Namespaces to be selected for PodMonitor discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            podMonitorNamespaceSelector: {}

            ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the probes created
            ##
            probeSelectorNilUsesHelmValues: true

            ## Probes to be selected for target discovery.
            ## If {}, select all Probes
            ##
            probeSelector: {}
            ## Example which selects Probes with label "prometheus" set to "somelabel"
            # probeSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## Namespaces to be selected for Probe discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            probeNamespaceSelector: {}
            ## Maximum size of metrics
            ##
            retentionSize: ""

            ## Enable compression of the write-ahead log using Snappy.
            ##
            walCompression: true

            ## If true, the Operator won't process any Prometheus configuration changes
            ##
            paused: false

            ## Number of replicas of each shard to deploy for a Prometheus deployment.
            ## Number of replicas multiplied by shards is the total number of Pods created.
            ##
            replicas: 1

            ## EXPERIMENTAL: Number of shards to distribute targets onto.
            ## Number of replicas multiplied by shards is the total number of Pods created.
            ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
            ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
            ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
            ## Sharding is done on the content of the `__address__` target meta-label.
            ##
            shards: 1

            ## Log level for Prometheus be configured in
            ##
            logLevel: info

            ## Log format for Prometheus be configured in
            ##
            logFormat: logfmt

            ## Prefix used to register routes, overriding externalUrl route.
            ## Useful for proxies that rewrite URLs.
            ##
            routePrefix: /

            ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
            ## Metadata Labels and Annotations gets propagated to the prometheus pods.
            ##
            podMetadata: {}
            # labels:
            #   app: prometheus
            #   k8s-app: prometheus

            ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
            ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
            ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
            ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
            podAntiAffinity: ""

            ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
            ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
            ##
            podAntiAffinityTopologyKey: kubernetes.io/hostname

            ## Assign custom affinity rules to the prometheus instance
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
            ##
            affinity: {}
            # nodeAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     nodeSelectorTerms:
            #     - matchExpressions:
            #       - key: kubernetes.io/e2e-az-name
            #         operator: In
            #         values:
            #         - e2e-az1
            #         - e2e-az2

            ## The remote_read spec configuration for Prometheus.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec
            remoteRead: []
            # - url: http://remote1/read
            ## additionalRemoteRead is appended to remoteRead
            additionalRemoteRead: []

            ## The remote_write spec configuration for Prometheus.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec
            remoteWrite: []
            # - url: http://remote1/push
            ## additionalRemoteWrite is appended to remoteWrite
            additionalRemoteWrite: []

            ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
            remoteWriteDashboards: false

            resources:
              limits:
                cpu: 500m
                memory: 1Gi
              requests:
                cpu: 300m
                memory: 512Mi

            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi

            # Additional volumes on the output StatefulSet definition.
            volumes: []

            # Additional VolumeMounts on the output StatefulSet definition.
            volumeMounts: []

            ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
            ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
            ## as specified in the official Prometheus documentation:
            ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
            ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
            ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
            ## scrape configs are going to break Prometheus after the upgrade.
            ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
            ##
            ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
            ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
            ##
            additionalScrapeConfigs:
              - job_name: 'unifipoller'
                scrape_interval: 30s
                static_configs:
                - targets: ['unifi-poller-svc.unifi.svc.cluster.local:9130']

            ## If scrape config contains a repetitive section, you may want to use a template.
            ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
            # additionalScrapeConfigs: |
            #  - job_name: "node-exporter"
            #    gce_sd_configs:
            #    {{range $zone := .Values.gcp_zones}}
            #    - project: "project1"
            #      zone: "{{$zone}}"
            #      port: 9100
            #    {{end}}
            #    relabel_configs:
            #    ...


            ## If additional scrape configurations are already deployed in a single secret file you can use this section.
            ## Expected values are the secret name and key
            ## Cannot be used with additionalScrapeConfigs
            additionalScrapeConfigsSecret: {}
              # enabled: false
              # name:
              # key:

            ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
            ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
            additionalPrometheusSecretsAnnotations: {}

            ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
            ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
            ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
            ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
            ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
            ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
            ##
            additionalAlertManagerConfigs: []
            # - consul_sd_configs:
            #   - server: consul.dev.test:8500
            #     scheme: http
            #     datacenter: dev
            #     tag_separator: ','
            #     services:
            #       - metrics-prometheus-alertmanager

            ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
            ## them separately from the helm deployment, you can use this section.
            ## Expected values are the secret name and key
            ## Cannot be used with additionalAlertManagerConfigs
            additionalAlertManagerConfigsSecret: {}
              # name:
              # key:
              # optional: false

            ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
            ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
            ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
            ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
            ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
            ## configs are going to break Prometheus after the upgrade.
            ##
            additionalAlertRelabelConfigs: []
            # - separator: ;
            #   regex: prometheus_replica
            #   replacement: $1
            #   action: labeldrop

            ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
            ## them separately from the helm deployment, you can use this section.
            ## Expected values are the secret name and key
            ## Cannot be used with additionalAlertRelabelConfigs
            additionalAlertRelabelConfigsSecret: {}
              # name:
              # key:

            ## SecurityContext holds pod-level security attributes and common container settings.
            ## This defaults to non root user with uid 1000 and gid 2000.
            ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md
            ##
            securityContext:
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
              fsGroup: 2000

            ## Priority class assigned to the Pods
            ##
            priorityClassName: ""

            ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
            ## This section is experimental, it may change significantly without deprecation notice in any release.
            ## This is experimental and may change significantly without backward compatibility in any release.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec
            ##
            thanos: {}
              # secretProviderClass:
              #   provider: gcp
              #   parameters:
              #     secrets: |
              #       - resourceName: "projects/$PROJECT_ID/secrets/testsecret/versions/latest"
              #         fileName: "objstore.yaml"
              # objectStorageConfigFile: /var/secrets/object-store.yaml

            ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
            ## if using proxy extraContainer update targetPort with proxy container port
            containers: []

            ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
            ## (permissions, dir tree) on mounted volumes before starting prometheus
            initContainers: []

            ## PortName to use for Prometheus.
            ##
            portName: "http-web"

            ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files
            ## on the file system of the Prometheus container e.g. bearer token files.
            arbitraryFSAccessThroughSMs: false

            ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor
            ## or PodMonitor to true, this overrides honor_labels to false.
            overrideHonorLabels: false

            ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.
            overrideHonorTimestamps: false

            ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor
            ## configs, and they will only discover endpoints within their current namespace. Defaults to false.
            ignoreNamespaceSelectors: false

            ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
            ## The label value will always be the namespace of the object that is being created.
            ## Disabled by default
            enforcedNamespaceLabel: ""

            ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.
            ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair
            ## Deprecated, use `excludedFromEnforcement` instead
            prometheusRulesExcludedFromEnforce: []

            ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects
            ## to be excluded from enforcing a namespace label of origin.
            ## Works only if enforcedNamespaceLabel set to true.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference
            excludedFromEnforcement: []

            ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,
            ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such
            ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions
            ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
            queryLogFile: false

            ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
            ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
            ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
            enforcedSampleLimit: false

            ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set
            ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall
            ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except
            ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.
            enforcedTargetLimit: false


            ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present
            ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
            ## 2.27.0 and newer.
            enforcedLabelLimit: false

            ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number
            ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
            ## 2.27.0 and newer.
            enforcedLabelNameLengthLimit: false

            ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this
            ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus
            ## versions 2.27.0 and newer.
            enforcedLabelValueLengthLimit: false

            ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental
            ## in Prometheus so it may change in any upcoming release.
            allowOverlappingBlocks: false

            ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
            ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
            minReadySeconds: 0

          additionalRulesForClusterRole: []
          #  - apiGroups: [ "" ]
          #    resources:
          #      - nodes/proxy
          #    verbs: [ "get", "list", "watch" ]

          additionalServiceMonitors: []
          ## Name of the ServiceMonitor to create
          ##
          # - name: ""

            ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
            ## the chart
            ##
            # additionalLabels: {}

            ## Service label for use in assembling a job name of the form <label value>-<port>
            ## If no label is specified, the service name is used.
            ##
            # jobLabel: ""

            ## labels to transfer from the kubernetes service to the target
            ##
            # targetLabels: []

            ## labels to transfer from the kubernetes pods to the target
            ##
            # podTargetLabels: []

            ## Label selector for services to which this ServiceMonitor applies
            ##
            # selector: {}

            ## Namespaces from which services are selected
            ##
            # namespaceSelector:
              ## Match any namespace
              ##
              # any: false

              ## Explicit list of namespace names to select
              ##
              # matchNames: []

            ## Endpoints of the selected service to be monitored
            ##
            # endpoints: []
              ## Name of the endpoint's service port
              ## Mutually exclusive with targetPort
              # - port: ""

              ## Name or number of the endpoint's target port
              ## Mutually exclusive with port
              # - targetPort: ""

              ## File containing bearer token to be used when scraping targets
              ##
              #   bearerTokenFile: ""

              ## Interval at which metrics should be scraped
              ##
              #   interval: 30s

              ## HTTP path to scrape for metrics
              ##
              #   path: /metrics

              ## HTTP scheme to use for scraping
              ##
              #   scheme: http

              ## TLS configuration to use when scraping the endpoint
              ##
              #   tlsConfig:

                  ## Path to the CA file
                  ##
                  # caFile: ""

                  ## Path to client certificate file
                  ##
                  # certFile: ""

                  ## Skip certificate verification
                  ##
                  # insecureSkipVerify: false

                  ## Path to client key file
                  ##
                  # keyFile: ""

                  ## Server name used to verify host name
                  ##
                  # serverName: ""

          additionalPodMonitors: []
          ## Name of the PodMonitor to create
          ##
          # - name: ""

            ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
            ## the chart
            ##
            # additionalLabels: {}

            ## Pod label for use in assembling a job name of the form <label value>-<port>
            ## If no label is specified, the pod endpoint name is used.
            ##
            # jobLabel: ""

            ## Label selector for pods to which this PodMonitor applies
            ##
            # selector: {}

            ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
            ##
            # podTargetLabels: {}

            ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
            ##
            # sampleLimit: 0

            ## Namespaces from which pods are selected
            ##
            # namespaceSelector:
              ## Match any namespace
              ##
              # any: false

              ## Explicit list of namespace names to select
              ##
              # matchNames: []

            ## Endpoints of the selected pods to be monitored
            ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint
            ##
            # podMetricsEndpoints: []

        ## Configuration for thanosRuler
        ## ref: https://thanos.io/tip/components/rule.md/
        ##
        thanosRuler:

          ## Deploy thanosRuler
          ##
          enabled: false

          ## Annotations for ThanosRuler
          ##
          annotations: {}

          ## Service account for ThanosRuler to use.
          ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
          ##
          serviceAccount:
            create: true
            name: ""
            annotations: {}

          ## Configure pod disruption budgets for ThanosRuler
          ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
          ## This configuration is immutable once created and will require the PDB to be deleted to be changed
          ## https://github.com/kubernetes/kubernetes/issues/45398
          ##
          podDisruptionBudget:
            enabled: false
            minAvailable: 1
            maxUnavailable: ""

          ingress:
            enabled: false

            # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
            # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
            # ingressClassName: nginx

            annotations: {}

            labels: {}

            ## Hosts must be provided if Ingress is enabled.
            ##
            hosts: []
              # - thanosruler.domain.com

            ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix
            ##
            paths: []
            # - /

            ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
            ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
            # pathType: ImplementationSpecific

            ## TLS configuration for ThanosRuler Ingress
            ## Secret must be manually created in the namespace
            ##
            tls: []
            # - secretName: thanosruler-general-tls
            #   hosts:
            #   - thanosruler.example.com

          ## Configuration for ThanosRuler service
          ##
          service:
            annotations: {}
            labels: {}
            clusterIP: ""

            ## Port for ThanosRuler Service to listen on
            ##
            port: 10902
            ## To be used with a proxy extraContainer port
            ##
            targetPort: 10902
            ## Port to expose on each node
            ## Only used if service.type is 'NodePort'
            ##
            nodePort: 30905
            ## List of IP addresses at which the Prometheus server service is available
            ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
            ##

            ## Additional ports to open for ThanosRuler service
            additionalPorts: []

            externalIPs: []
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster
            type: ClusterIP
          serviceMonitor:
            ## Scrape interval. If not set, the Prometheus default scrape interval is used.
            ##
            interval: ""
            selfMonitor: true

            ## proxyUrl: URL of a proxy that should be used for scraping.
            ##
            proxyUrl: ""

            ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
            scheme: ""

            ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
            ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
            tlsConfig: {}

            bearerTokenFile:

            ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            metricRelabelings: []
            # - action: keep
            #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
            #   sourceLabels: [__name__]

            ## RelabelConfigs to apply to samples before scraping
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            relabelings: []
            # - sourceLabels: [__meta_kubernetes_pod_node_name]
            #   separator: ;
            #   regex: ^(.*)$
            #   targetLabel: nodename
            #   replacement: $1
            #   action: replace

          ## Settings affecting thanosRulerpec
          ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec
          ##
          thanosRulerSpec:
            ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
            ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.
            ##
            podMetadata: {}

            ## Image of ThanosRuler
            ##
            image:
              repository: quay.io/thanos/thanos
              tag: v0.28.1
              sha: ""

            ## Namespaces to be selected for PrometheusRules discovery.
            ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            ruleNamespaceSelector: {}

            ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the PrometheusRule resources created
            ##
            ruleSelectorNilUsesHelmValues: true

            ## PrometheusRules to be selected for target discovery.
            ## If {}, select all PrometheusRules
            ##
            ruleSelector: {}
            ## Example which select all PrometheusRules resources
            ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
            # ruleSelector:
            #   matchExpressions:
            #     - key: prometheus
            #       operator: In
            #       values:
            #         - example-rules
            #         - example-rules-2
            #
            ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
            # ruleSelector:
            #   matchLabels:
            #     role: example-rules

            ## Define Log Format
            # Use logfmt (default) or json logging
            logFormat: logfmt

            ## Log level for ThanosRuler to be configured with.
            ##
            logLevel: info

            ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the
            ## running cluster equal to the expected size.
            replicas: 1

            ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression
            ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
            ##
            retention: 24h

            ## Interval between consecutive evaluations.
            ##
            evaluationInterval: ""

            ## Storage is the definition of how storage will be used by the ThanosRuler instances.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
            ##
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi

            ## AlertmanagerConfig define configuration for connecting to alertmanager.
            ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.
            alertmanagersConfig: {}
            #   - api_version: v2
            #     http_config:
            #       basic_auth:
            #         username: some_user
            #         password: some_pass
            #     static_configs:
            #       - alertmanager.thanos.io
            #     scheme: http
            #     timeout: 10s

            ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.
            ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.
            # alertmanagersUrl:

            ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false
            ##
            externalPrefix:

            ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
            ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
            ##
            routePrefix: /

            ## ObjectStorageConfig configures object storage in Thanos. Alternative to
            ## ObjectStorageConfigFile, and lower order priority.
            objectStorageConfig: {}

            ## ObjectStorageConfigFile specifies the path of the object storage configuration file.
            ## When used alongside with ObjectStorageConfig, ObjectStorageConfigFile takes precedence.
            objectStorageConfigFile: ""

            ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.
            ## Maps to the --query flag of thanos ruler.
            queryEndpoints: []

            ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
            ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
            queryConfig: {}

            ## Labels configure the external label pairs to ThanosRuler. A default replica
            ## label `thanos_ruler_replica` will be always added as a label with the value
            ## of the pod's name and it will be dropped in the alerts.
            labels: {}

            ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
            ##
            paused: false

            ## Define which Nodes the Pods are scheduled on.
            ## ref: https://kubernetes.io/docs/user-guide/node-selection/
            ##
            nodeSelector:
              kubernetes.io/hostname: node-two

            ## Define resources requests and limits for single Pods.
            ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
            ##
            resources:
              limits:
                cpu: 500m
                memory: 1Gi
              requests:
                cpu: 250m
                memory: 512Mi

            ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
            ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
            ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
            ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
            ##
            podAntiAffinity: ""

            ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
            ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
            ##
            podAntiAffinityTopologyKey: kubernetes.io/hostname

            ## Assign custom affinity rules to the thanosRuler instance
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
            ##
            affinity: {}
            # nodeAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     nodeSelectorTerms:
            #     - matchExpressions:
            #       - key: kubernetes.io/e2e-az-name
            #         operator: In
            #         values:
            #         - e2e-az1
            #         - e2e-az2

            ## If specified, the pod's tolerations.
            ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
            ##
            tolerations: []
            # - key: "key"
            #   operator: "Equal"
            #   value: "value"
            #   effect: "NoSchedule"

            ## If specified, the pod's topology spread constraints.
            ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
            ##
            topologySpreadConstraints: []
            # - maxSkew: 1
            #   topologyKey: topology.kubernetes.io/zone
            #   whenUnsatisfiable: DoNotSchedule
            #   labelSelector:
            #     matchLabels:
            #       app: thanos-ruler

            ## SecurityContext holds pod-level security attributes and common container settings.
            ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
            ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
            ##
            securityContext:
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
              fsGroup: 2000

            ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.
            ## Note this is only for the ThanosRuler UI, not the gossip communication.
            ##
            listenLocal: false

            ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.
            ##
            containers: []

            # Additional volumes on the output StatefulSet definition.
            volumes: []

            # Additional VolumeMounts on the output StatefulSet definition.
            volumeMounts: []

            ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
            ## (permissions, dir tree) on mounted volumes before starting prometheus
            initContainers: []

            ## Priority class assigned to the Pods
            ##
            priorityClassName: ""

            ## PortName to use for ThanosRuler.
            ##
            portName: "web"

          ## ExtraSecret can be used to store various data in an extra secret
          ## (use it for example to store hashed basic auth credentials)
          extraSecret:
            ## if not set, name will be auto generated
            # name: ""
            annotations: {}
            data: {}
          #   auth: |
          #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
          #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

        ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
        ##
        cleanPrometheusOperatorObjectNames: false

  destination:
    namespace: monitoring
    name: in-cluster
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
      allowEmpty: true
    syncOptions:
    - Validate=false
    - CreateNamespace=false
    - PrunePropagationPolicy=foreground
    - PruneLast=true
    - ApplyOutOfSyncOnly=false
    - Prune=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
