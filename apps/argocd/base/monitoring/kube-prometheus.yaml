apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus
spec:
  project: monitoring
  source:
    repoURL: 'https://github.com/prometheus-community/helm-charts.git'
    targetRevision: kube-prometheus-stack-43.2.1
    path: charts/kube-prometheus-stack
    helm:
      skipCrds: true
      releaseName: kube-prometheus
      values: |
        nameOverride: ""
        namespaceOverride: "monitoring"
        kubeTargetVersionOverride: ""
        kubeVersionOverride: ""
        fullnameOverride: ""
        commonLabels: {}
        defaultRules:
          create: true
          rules:
            etcd: false
            k8s: false
            kubeApiserverAvailability: false
            kubeApiserverBurnrate: false
            kubeApiserverHistogram: false
            kubeApiserverSlos: false
            kubeControllerManager: false
            kubelet: false
            kubeProxy: false
            kubeScheduler: false
            alertmanager: true
            configReloaders: true
            general: true
            kubePrometheusGeneral: true
            kubePrometheusNodeRecording: true
            kubernetesApps: true
            kubernetesResources: true
            kubernetesStorage: true
            kubernetesSystem: true
            kubeSchedulerAlerting: true
            kubeSchedulerRecording: true
            kubeStateMetrics: true
            network: true
            node: true
            nodeExporterAlerting: true
            nodeExporterRecording: true
            prometheus: true
            prometheusOperator: true
          appNamespacesTarget: ".*"
          labels: {}
          annotations: {}
          additionalRuleLabels: {}
          additionalRuleAnnotations: {}
          runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
          disabled: {}
        additionalPrometheusRulesMap: {}
        global:
          rbac:
            create: true
            createAggregateClusterRoles: false
            pspEnabled: false
            pspAnnotations: {}
          imagePullSecrets: []
        alertmanager:
          enabled: false
          annotations: {}
          apiVersion: v2
          serviceAccount:
            create: true
            name: ""
            annotations: {}
          podDisruptionBudget:
            enabled: false
            minAvailable: 1
            maxUnavailable: ""
          config:
            global:
              resolve_timeout: 5m
            inhibit_rules:
              - source_matchers:
                  - 'severity = critical'
                target_matchers:
                  - 'severity =~ warning|info'
                equal:
                  - 'namespace'
                  - 'alertname'
              - source_matchers:
                  - 'severity = warning'
                target_matchers:
                  - 'severity = info'
                equal:
                  - 'namespace'
                  - 'alertname'
              - source_matchers:
                  - 'alertname = InfoInhibitor'
                target_matchers:
                  - 'severity = info'
                equal:
                  - 'namespace'
            route:
              group_by: ['namespace']
              group_wait: 30s
              group_interval: 5m
              repeat_interval: 12h
              receiver: 'null'
              routes:
              - receiver: 'null'
                matchers:
                  - alertname =~ "InfoInhibitor|Watchdog"
            receivers:
            - name: 'null'
            templates:
            - '/etc/alertmanager/config/*.tmpl'
          tplConfig: false
          templateFiles: {}
          ingress:
            enabled: false
          secret:
            annotations: {}
          ingressPerReplica:
            enabled: false
          service:
            annotations: {}
            labels: {}
            clusterIP: ""

            ## Port for Alertmanager Service to listen on
            ##
            port: 9093
            ## To be used with a proxy extraContainer port
            ##
            targetPort: 9093
            ## Port to expose on each node
            ## Only used if service.type is 'NodePort'
            ##
            nodePort: 30903
            ## List of IP addresses at which the Prometheus server service is available
            ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
            ##

            ## Additional ports to open for Alertmanager service
            additionalPorts: []
            # additionalPorts:
            # - name: authenticated
            #   port: 8081
            #   targetPort: 8081

            externalIPs: []
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

          ## Configuration for creating a separate Service for each statefulset Alertmanager replica
          ##
          servicePerReplica:
            enabled: false
            annotations: {}

            ## Port for Alertmanager Service per replica to listen on
            ##
            port: 9093

            ## To be used with a proxy extraContainer port
            targetPort: 9093

            ## Port to expose on each node
            ## Only used if servicePerReplica.type is 'NodePort'
            ##
            nodePort: 30904

            ## Loadbalancer source IP ranges
            ## Only used if servicePerReplica.type is "LoadBalancer"
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster

            ## Service type
            ##
            type: ClusterIP

          ## If true, create a serviceMonitor for alertmanager
          ##
          serviceMonitor:
            ## Scrape interval. If not set, the Prometheus default scrape interval is used.
            ##
            interval: ""
            selfMonitor: true

            ## proxyUrl: URL of a proxy that should be used for scraping.
            ##
            proxyUrl: ""

            ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
            scheme: ""

            ## enableHttp2: Whether to enable HTTP2.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
            enableHttp2: false

            ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
            ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
            tlsConfig: {}

            bearerTokenFile:

            ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            metricRelabelings: []
            # - action: keep
            #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
            #   sourceLabels: [__name__]

            ## RelabelConfigs to apply to samples before scraping
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            relabelings: []
            # - sourceLabels: [__meta_kubernetes_pod_node_name]
            #   separator: ;
            #   regex: ^(.*)$
            #   targetLabel: nodename
            #   replacement: $1
            #   action: replace

          ## Settings affecting alertmanagerSpec
          ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
          ##
          alertmanagerSpec:
            podMetadata: {}
            image:
              registry: quay.io
              repository: prometheus/alertmanager
              tag: v0.25.0
              sha: ""
            ## If true then the user will be responsible to provide a secret with alertmanager configuration
            ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
            ##
            useExistingSecret: false

            ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
            ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
            ##
            secrets: []

            ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
            ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
            ##
            configMaps: []

            ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
            ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
            ##
            # configSecret:

            ## WebTLSConfig defines the TLS parameters for HTTPS
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec
            web: {}

            ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
            ##
            alertmanagerConfigSelector: {}
            ## Example which selects all alertmanagerConfig resources
            ## with label "alertconfig" with values any of "example-config" or "example-config-2"
            # alertmanagerConfigSelector:
            #   matchExpressions:
            #     - key: alertconfig
            #       operator: In
            #       values:
            #         - example-config
            #         - example-config-2
            #
            ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
            # alertmanagerConfigSelector:
            #   matchLabels:
            #     role: example-config
            alertmanagerConfigNamespaceSelector: {}
            alertmanagerConfiguration: {}
            logFormat: logfmt
            logLevel: info
            replicas: 1
            retention: 120h
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi
            externalUrl:
            routePrefix: /
            paused: false
            nodeSelector:
              kubernetes.io/hostname: node-two
            ## Define resources requests and limits for single Pods.
            ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
            ##
            resources:
              limits:
                cpu: 300m
                memory: 720Mi
              requests:
                cpu: 150m
                memory: 512Mi
            podAntiAffinity: ""
            podAntiAffinityTopologyKey: kubernetes.io/hostname
            affinity: {}
            tolerations: []
            topologySpreadConstraints: []
            securityContext:
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
              fsGroup: 2000
            listenLocal: false
            containers: []
            volumes: []
            # Additional VolumeMounts on the output StatefulSet definition.
            volumeMounts: []

            ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
            ## (permissions, dir tree) on mounted volumes before starting prometheus
            initContainers: []

            ## Priority class assigned to the Pods
            ##
            priorityClassName: ""

            ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
            ##
            additionalPeers: []

            ## PortName to use for Alert Manager.
            ##
            portName: "http-web"

            ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
            ##
            clusterAdvertiseAddress: false

            ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
            ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
            forceEnableClusterMode: false

            ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
            ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
            minReadySeconds: 0

          ## ExtraSecret can be used to store various data in an extra secret
          ## (use it for example to store hashed basic auth credentials)
          extraSecret:
            ## if not set, name will be auto generated
            # name: ""
            annotations: {}
            data: {}
          #   auth: |
          #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
          #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.
        grafana:
          enabled: true
          namespaceOverride: monitoring
          plugins:
            - grafana-piechart-panel
            - grafana-clock-panel
          dashboardProviders:
            dashboardproviders.yaml:
              apiVersion: 1
              providers:
              - name: unifi
                orgId: 1
                folder: 'Unifi'
                type: file
                disableDeletion: true
                editable: true
                options:
                  path: /var/lib/grafana/dashboards/unifi
              - name: cluster
                orgId: 1
                folder: 'cluster'
                type: file
                disableDeletion: true
                editable: true
                options:
                  path: /var/lib/grafana/dashboards/cluster
          dashboards:
            unifi:
              unifiPollerClientDPI:
                gnetId: 11310
                revision: 4
                datasource: Prometheus
              unifiPollerClientInsights:
                gnetId: 11315
                revision: 8
                datasource: Prometheus
              unifiPollerNetworkSites:
                gnetId: 11311
                revision: 4
                datasource: Prometheus
              unifiPollerUAPInsights:
                gnetId: 11314
                revision: 8
                datasource: Prometheus
              unifiPollerUSGInsights:
                gnetId: 11313
                revision: 8
                datasource: Prometheus
              unifiPollerUSWInsights:
                gnetId: 11312
                revision: 8
                datasource: Prometheus
            cluster:
              longhornMonitoring:
                gnetId: 16888
                revision: 5
                datasource: Prometheus
              argocdMonitoring:
                gnetId: 14584
                revision: 1
                datasource: Prometheus
              postgresMonitoring:
                gnetId: 12485
                revision: 1
                datasource: Prometheus
              adguardMonitoring:
                gnetId: 13330
                revision: 3
                datasource: Prometheus
              nvidiaMonitoring:
                gnetId: 14574
                revision: 8
                datasource: Prometheus
              vaultMonitoring:
                gnetId: 15124
                revision: 2
                datasource: Prometheus
          forceDeployDatasources: false
          forceDeployDashboards: false
          defaultDashboardsEnabled: true
          defaultDashboardsTimezone: "America/Sao_Paulo"
          adminPassword: <path:kv/data/kube-prometheus#grafana-admin-password>
          rbac:
            pspEnabled: false
          ingress:
            enabled: false
          sidecar:
            dashboards:
              enabled: true
              labelValue: "1"
              folderAnnotation: grafana_folder
              searchNamespace: "ALL"
              multicluster:
                global:
                  enabled: false
                etcd:
                  enabled: false
              provider:
                foldersFromFilesStructure: true
            datasources:
              enabled: true
              defaultDatasourceEnabled: true
              uid: prometheus
              # url: http://prometheus-stack-prometheus:9090/
              annotations: {}
              createPrometheusReplicasDatasources: false
              label: grafana_datasource
              labelValue: "1"
              exemplarTraceIdDestinations: {}
          extraConfigmapMounts: []
          deleteDatasources: []
          additionalDataSources: []
          service:
            portName: http-web
          serviceMonitor:
            enabled: true
            path: "/metrics"
            namespace: monitoring
            labels: {}
            interval: ""
            scheme: http
            tlsConfig: {}
            scrapeTimeout: 30s
            relabelings: []
        kubeApiServer:
          enabled: true
        kubelet:
          enabled: true
        kubeControllerManager:
          enabled: true
        coreDns:
          enabled: true
          service:
            port: 9153
            targetPort: 9153
          serviceMonitor:
            interval: ""
            proxyUrl: ""
            metricRelabelings: []
            relabelings: []
            additionalLabels: {}
        kubeDns:
          enabled: true
        kubeEtcd:
          enabled: false
        kubeScheduler:
          enabled: true
        kubeProxy:
          enabled: true
        kubeStateMetrics:
          enabled: true
        kube-state-metrics:
          namespaceOverride: monitoring
          rbac:
            create: true
          releaseLabel: true
          prometheus:
            monitor:
              enabled: true
              interval: ""
              scrapeTimeout: ""
              proxyUrl: ""
              honorLabels: true
              metricRelabelings: []
              relabelings: []
          selfMonitor:
            enabled: true
        nodeExporter:
          enabled: true
        prometheus-node-exporter:
          namespaceOverride: monitoring
          podLabels:
            jobLabel: node-exporter
          releaseLabel: true
          extraArgs:
            - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
            - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
          service:
            portName: http-metrics
          prometheus:
            monitor:
              enabled: true
              jobLabel: jobLabel
              interval: ""
              scrapeTimeout: ""
              proxyUrl: ""
              metricRelabelings: []
              relabelings: []
          rbac:
            pspEnabled: false
        prometheusOperator:
          enabled: true
          tls:
            enabled: false
            tlsMinVersion: VersionTLS13
            internalPort: 10250
          admissionWebhooks:
            failurePolicy: Fail
            timeoutSeconds: 10
            enabled: false
            caBundle: ""
            annotations:
              argocd.argoproj.io/hook: PreSync
              argocd.argoproj.io/hook-delete-policy: HookSucceeded
            patch:
              enabled: true
              image:
                repository: k8s.gcr.io/ingress-nginx/kube-webhook-certgen
                tag: v1.3.0
                sha: ""
                pullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 400m
                  memory: 512Mi
                requests:
                  cpu: 200m
                  memory: 256Mi
              priorityClassName: ""
              annotations:
                argocd.argoproj.io/hook: PreSync
                argocd.argoproj.io/hook-delete-policy: HookSucceeded
              podAnnotations: {}
              nodeSelector:
                kubernetes.io/hostname: node-two
              affinity: {}
              tolerations: []
              securityContext:
                runAsGroup: 2000
                runAsNonRoot: true
                runAsUser: 2000
            createSecretJob:
              securityContext: {}
            patchWebhookJob:
              securityContext: {}
            certManager:
              enabled: false
          namespaces: {}
            # releaseNamespace: true
            # additional:
            # - kube-system
          denyNamespaces: []
          alertmanagerInstanceNamespaces: []
          alertmanagerConfigNamespaces: []
          prometheusInstanceNamespaces: []
          thanosRulerInstanceNamespaces: []
          # clusterDomain: "cluster.local"
          networkPolicy:
            enabled: false
          serviceAccount:
            create: true
            name: ""
          service:
            annotations: {}
            labels: {}
            clusterIP: ""
            nodePort: 30080
            nodePortTls: 30443
            additionalPorts: []
            loadBalancerIP: ""
            loadBalancerSourceRanges: []
            externalTrafficPolicy: Cluster
            type: ClusterIP
            externalIPs: []
          labels: {}
          annotations: {}
          podLabels: {}
          podAnnotations: {}
          kubeletService:
            enabled: false
          serviceMonitor:
            relabelings:
            - sourceLabels:
              - __address__
              action: replace
              targetLabel: job
              replacement: prometheus-operator
            interval: ""
            scrapeTimeout: ""
            selfMonitor: true
            metricRelabelings: []
          resources:
            limits:
              cpu: 350m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          hostNetwork: false
          nodeSelector:
            kubernetes.io/hostname: node-two
          tolerations: []
          affinity: {}
          dnsConfig: {}
          securityContext:
            fsGroup: 65534
            runAsGroup: 65534
            runAsNonRoot: true
            runAsUser: 65534
          containerSecurityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
          verticalPodAutoscaler:
            enabled: false
          image:
            registry: quay.io
            repository: prometheus-operator/prometheus-operator
            tag: v0.61.1
            sha: ""
            pullPolicy: IfNotPresent
            sha: ""
            pullPolicy: IfNotPresent
          prometheusConfigReloader:
            image:
              repository: quay.io/prometheus-operator/prometheus-config-reloader
              tag: v0.61.1
              sha: ""
            resources:
              requests:
                cpu: 250m
                memory: 256Mi
              limits:
                cpu: 450m
                memory: 512Mi
          thanosImage:
            repository: quay.io/thanos/thanos
            tag: v0.29.0
            sha: ""
          secretFieldSelector: ""
        prometheus:
          enabled: true
          annotations: {}
          serviceAccount:
            create: true
            name: ""
            annotations: {}
          thanosService:
            enabled: false
            annotations: {}
            labels: {}
            externalTrafficPolicy: Cluster
            type: ClusterIP
            portName: grpc
            port: 10901
            targetPort: "grpc"
            httpPortName: http
            httpPort: 10902
            targetHttpPort: "http"
            clusterIP: "None"
            nodePort: 30901
            httpNodePort: 30902
          thanosServiceMonitor:
            enabled: false
            interval: ""
            scheme: ""
            tlsConfig: {}
            bearerTokenFile:
            metricRelabelings: []
            relabelings: []
          thanosServiceExternal:
            enabled: false
            annotations: {}
            labels: {}
            loadBalancerIP: ""
            loadBalancerSourceRanges: []
            portName: grpc
            port: 10901
            targetPort: "grpc"
            httpPortName: http
            httpPort: 10902
            targetHttpPort: "http"
            externalTrafficPolicy: Cluster
            type: LoadBalancer
            nodePort: 30901
            httpNodePort: 30902
          service:
            annotations: {}
            labels: {}
            clusterIP: ""
            port: 9090
            targetPort: 9090
            externalIPs: []
            nodePort: 30090
            loadBalancerIP: ""
            loadBalancerSourceRanges: []
            externalTrafficPolicy: Cluster
            type: ClusterIP
            additionalPorts: []
            publishNotReadyAddresses: false
            sessionAffinity: ""
          servicePerReplica:
            enabled: false
            annotations: {}
            port: 9090
            targetPort: 9090
            nodePort: 30091
            loadBalancerSourceRanges: []
            externalTrafficPolicy: Cluster
            type: ClusterIP
          podDisruptionBudget:
            enabled: false
            minAvailable: 1
            maxUnavailable: ""
          thanosIngress:
            enabled: false
            annotations: {}
            labels: {}
            servicePort: 10901
            nodePort: 30901
            hosts: []
            paths: []
            tls: []
          extraSecret:
            ## if not set, name will be auto generated
            # name: ""
            annotations: {}
            data: {}
          #   auth: |
          #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
          #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

          ingress:
            enabled: false
            annotations: {}
            labels: {}
            hosts: []
            paths: []
            tls: []
          ingressPerReplica:
            enabled: false
            annotations: {}
            labels: {}
            hostPrefix: ""
            hostDomain: ""
            paths: []
            tlsSecretName: ""
            tlsSecretPerReplica:
              enabled: false
              prefix: "prometheus"
          podSecurityPolicy:
            allowedCapabilities: []
            allowedHostPaths: []
            volumes: []
          serviceMonitor:
            interval: ""
            selfMonitor: true
            scheme: ""
            tlsConfig: {}
            bearerTokenFile:
            metricRelabelings: []
            relabelings: []
          prometheusSpec:
            retention: 3d
            disableCompaction: false
            apiserverConfig: {}
            additionalArgs: []
            scrapeInterval: ""
            scrapeTimeout: ""
            evaluationInterval: ""
            listenLocal: false
            enableAdminAPI: false
            web: {}
            exemplars: ""
            enableFeatures: []
            image:
              registry: quay.io
              repository: prometheus/prometheus
              tag: v2.41.0
              sha: ""
            tolerations: []
            topologySpreadConstraints: []
            alertingEndpoints: []
            externalLabels: {}
            enableRemoteWriteReceiver: false
            replicaExternalLabelName: ""
            replicaExternalLabelNameClear: false
            prometheusExternalLabelName: ""
            prometheusExternalLabelNameClear: false
            externalUrl: ""
            nodeSelector:
              kubernetes.io/hostname: node-two
            secrets: []
            configMaps: []
            query: {}
            ruleNamespaceSelector: {}
            ruleSelectorNilUsesHelmValues: true
            ruleSelector: {}
            serviceMonitorSelectorNilUsesHelmValues: true
            serviceMonitorSelector: {}
            ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
            # serviceMonitorSelector:
            #   matchLabels:
            #     prometheus: somelabel
            ## Namespaces to be selected for ServiceMonitor discovery.
            ##
            serviceMonitorNamespaceSelector: {}
            ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
            # serviceMonitorNamespaceSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the podmonitors created
            ##
            podMonitorSelectorNilUsesHelmValues: true

            ## PodMonitors to be selected for target discovery.
            ## If {}, select all PodMonitors
            ##
            podMonitorSelector: {}
            ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
            # podMonitorSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## Namespaces to be selected for PodMonitor discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            podMonitorNamespaceSelector: {}

            ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the probes created
            ##
            probeSelectorNilUsesHelmValues: true

            ## Probes to be selected for target discovery.
            ## If {}, select all Probes
            ##
            probeSelector: {}
            ## Example which selects Probes with label "prometheus" set to "somelabel"
            # probeSelector:
            #   matchLabels:
            #     prometheus: somelabel

            ## Namespaces to be selected for Probe discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            probeNamespaceSelector: {}
            ## Maximum size of metrics
            ##
            retentionSize: ""

            ## Enable compression of the write-ahead log using Snappy.
            ##
            walCompression: true

            ## If true, the Operator won't process any Prometheus configuration changes
            ##
            paused: false

            ## Number of replicas of each shard to deploy for a Prometheus deployment.
            ## Number of replicas multiplied by shards is the total number of Pods created.
            ##
            replicas: 1

            ## EXPERIMENTAL: Number of shards to distribute targets onto.
            ## Number of replicas multiplied by shards is the total number of Pods created.
            ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
            ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
            ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
            ## Sharding is done on the content of the `__address__` target meta-label.
            ##
            shards: 1

            ## Log level for Prometheus be configured in
            ##
            logLevel: info

            ## Log format for Prometheus be configured in
            ##
            logFormat: logfmt

            ## Prefix used to register routes, overriding externalUrl route.
            ## Useful for proxies that rewrite URLs.
            ##
            routePrefix: /

            ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
            ## Metadata Labels and Annotations gets propagated to the prometheus pods.
            ##
            podMetadata: {}
            # labels:
            #   app: prometheus
            #   k8s-app: prometheus

            ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
            ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
            ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
            ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
            podAntiAffinity: ""

            ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
            ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
            ##
            podAntiAffinityTopologyKey: kubernetes.io/hostname

            ## Assign custom affinity rules to the prometheus instance
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
            ##
            affinity: {}
            # nodeAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     nodeSelectorTerms:
            #     - matchExpressions:
            #       - key: kubernetes.io/e2e-az-name
            #         operator: In
            #         values:
            #         - e2e-az1
            #         - e2e-az2

            ## The remote_read spec configuration for Prometheus.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec
            remoteRead: []
            # - url: http://remote1/read
            ## additionalRemoteRead is appended to remoteRead
            additionalRemoteRead: []

            ## The remote_write spec configuration for Prometheus.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec
            remoteWrite: []
            # - url: http://remote1/push
            ## additionalRemoteWrite is appended to remoteWrite
            additionalRemoteWrite: []

            ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
            remoteWriteDashboards: false

            resources:
              limits:
                cpu: 500m
                memory: 1Gi
              requests:
                cpu: 300m
                memory: 512Mi

            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi
            # Additional volumes on the output StatefulSet definition.
            volumes: []

            # Additional VolumeMounts on the output StatefulSet definition.
            volumeMounts: []

            ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
            ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
            ## as specified in the official Prometheus documentation:
            ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
            ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
            ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
            ## scrape configs are going to break Prometheus after the upgrade.
            ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
            ##
            ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
            ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
            ##
            additionalScrapeConfigs:
              - job_name: 'unifipoller'
                scrape_interval: 30s
                static_configs:
                - targets: ['192.168.1.8:32767']
              - job_name: "hass"
                scrape_interval: 60s
                metrics_path: /api/prometheus
                authorization:
                  credentials: "<path:kv/data/ha#auth-token>"
                scheme: http
                static_configs:
                  - targets: ['192.168.1.10:8123']
              - job_name: 'adguard'
                static_configs:
                - targets: ['192.168.1.8:9617']
            ## If scrape config contains a repetitive section, you may want to use a template.
            ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
            # additionalScrapeConfigs: |
            #  - job_name: "node-exporter"
            #    gce_sd_configs:
            #    {{range $zone := .Values.gcp_zones}}
            #    - project: "project1"
            #      zone: "{{$zone}}"
            #      port: 9100
            #    {{end}}
            #    relabel_configs:
            #    ...


            ## If additional scrape configurations are already deployed in a single secret file you can use this section.
            ## Expected values are the secret name and key
            ## Cannot be used with additionalScrapeConfigs
            additionalScrapeConfigsSecret: {}
              # enabled: false
              # name:
              # key:

            ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
            ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
            additionalPrometheusSecretsAnnotations: {}

            ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
            ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
            ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
            ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
            ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
            ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
            ##
            additionalAlertManagerConfigs: []
            # - consul_sd_configs:
            #   - server: consul.dev.test:8500
            #     scheme: http
            #     datacenter: dev
            #     tag_separator: ','
            #     services:
            #       - metrics-prometheus-alertmanager

            ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
            ## them separately from the helm deployment, you can use this section.
            ## Expected values are the secret name and key
            ## Cannot be used with additionalAlertManagerConfigs
            additionalAlertManagerConfigsSecret: {}
              # name:
              # key:
              # optional: false

            ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
            ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
            ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
            ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
            ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
            ## configs are going to break Prometheus after the upgrade.
            ##
            additionalAlertRelabelConfigs: []
            # - separator: ;
            #   regex: prometheus_replica
            #   replacement: $1
            #   action: labeldrop

            ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
            ## them separately from the helm deployment, you can use this section.
            ## Expected values are the secret name and key
            ## Cannot be used with additionalAlertRelabelConfigs
            additionalAlertRelabelConfigsSecret: {}
              # name:
              # key:

            ## SecurityContext holds pod-level security attributes and common container settings.
            ## This defaults to non root user with uid 1000 and gid 2000.
            ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md
            ##
            securityContext:
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
              fsGroup: 2000

            ## Priority class assigned to the Pods
            ##
            priorityClassName: ""

            ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
            ## This section is experimental, it may change significantly without deprecation notice in any release.
            ## This is experimental and may change significantly without backward compatibility in any release.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec
            ##
            thanos: {}
              # secretProviderClass:
              #   provider: gcp
              #   parameters:
              #     secrets: |
              #       - resourceName: "projects/$PROJECT_ID/secrets/testsecret/versions/latest"
              #         fileName: "objstore.yaml"
              # objectStorageConfigFile: /var/secrets/object-store.yaml

            ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
            ## if using proxy extraContainer update targetPort with proxy container port
            containers: []

            ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
            ## (permissions, dir tree) on mounted volumes before starting prometheus
            initContainers: []

            ## PortName to use for Prometheus.
            ##
            portName: "http-web"

            ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files
            ## on the file system of the Prometheus container e.g. bearer token files.
            arbitraryFSAccessThroughSMs: false

            ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor
            ## or PodMonitor to true, this overrides honor_labels to false.
            overrideHonorLabels: false

            ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.
            overrideHonorTimestamps: false

            ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor
            ## configs, and they will only discover endpoints within their current namespace. Defaults to false.
            ignoreNamespaceSelectors: false

            ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
            ## The label value will always be the namespace of the object that is being created.
            ## Disabled by default
            enforcedNamespaceLabel: ""

            ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.
            ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair
            ## Deprecated, use `excludedFromEnforcement` instead
            prometheusRulesExcludedFromEnforce: []

            ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects
            ## to be excluded from enforcing a namespace label of origin.
            ## Works only if enforcedNamespaceLabel set to true.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference
            excludedFromEnforcement: []

            ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,
            ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such
            ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions
            ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
            queryLogFile: false

            ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
            ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
            ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
            enforcedSampleLimit: false

            ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set
            ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall
            ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except
            ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.
            enforcedTargetLimit: false


            ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present
            ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
            ## 2.27.0 and newer.
            enforcedLabelLimit: false

            ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number
            ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
            ## 2.27.0 and newer.
            enforcedLabelNameLengthLimit: false

            ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this
            ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus
            ## versions 2.27.0 and newer.
            enforcedLabelValueLengthLimit: false

            ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental
            ## in Prometheus so it may change in any upcoming release.
            allowOverlappingBlocks: false

            ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
            ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
            minReadySeconds: 0

          additionalRulesForClusterRole: []
          #  - apiGroups: [ "" ]
          #    resources:
          #      - nodes/proxy
          #    verbs: [ "get", "list", "watch" ]

          additionalServiceMonitors: []
          ## Name of the ServiceMonitor to create
          ##
          # - name: ""

            ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
            ## the chart
            ##
            # additionalLabels: {}

            ## Service label for use in assembling a job name of the form <label value>-<port>
            ## If no label is specified, the service name is used.
            ##
            # jobLabel: ""

            ## labels to transfer from the kubernetes service to the target
            ##
            # targetLabels: []

            ## labels to transfer from the kubernetes pods to the target
            ##
            # podTargetLabels: []

            ## Label selector for services to which this ServiceMonitor applies
            ##
            # selector: {}

            ## Namespaces from which services are selected
            ##
            # namespaceSelector:
              ## Match any namespace
              ##
              # any: false

              ## Explicit list of namespace names to select
              ##
              # matchNames: []

            ## Endpoints of the selected service to be monitored
            ##
            # endpoints: []
              ## Name of the endpoint's service port
              ## Mutually exclusive with targetPort
              # - port: ""

              ## Name or number of the endpoint's target port
              ## Mutually exclusive with port
              # - targetPort: ""

              ## File containing bearer token to be used when scraping targets
              ##
              #   bearerTokenFile: ""

              ## Interval at which metrics should be scraped
              ##
              #   interval: 30s

              ## HTTP path to scrape for metrics
              ##
              #   path: /metrics

              ## HTTP scheme to use for scraping
              ##
              #   scheme: http

              ## TLS configuration to use when scraping the endpoint
              ##
              #   tlsConfig:

                  ## Path to the CA file
                  ##
                  # caFile: ""

                  ## Path to client certificate file
                  ##
                  # certFile: ""

                  ## Skip certificate verification
                  ##
                  # insecureSkipVerify: false

                  ## Path to client key file
                  ##
                  # keyFile: ""

                  ## Server name used to verify host name
                  ##
                  # serverName: ""

          additionalPodMonitors: []
          ## Name of the PodMonitor to create
          ##
          # - name: ""

            ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
            ## the chart
            ##
            # additionalLabels: {}

            ## Pod label for use in assembling a job name of the form <label value>-<port>
            ## If no label is specified, the pod endpoint name is used.
            ##
            # jobLabel: ""

            ## Label selector for pods to which this PodMonitor applies
            ##
            # selector: {}

            ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
            ##
            # podTargetLabels: {}

            ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
            ##
            # sampleLimit: 0

            ## Namespaces from which pods are selected
            ##
            # namespaceSelector:
              ## Match any namespace
              ##
              # any: false

              ## Explicit list of namespace names to select
              ##
              # matchNames: []

            ## Endpoints of the selected pods to be monitored
            ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint
            ##
            # podMetricsEndpoints: []

        ## Configuration for thanosRuler
        ## ref: https://thanos.io/tip/components/rule.md/
        ##
        thanosRuler:

          ## Deploy thanosRuler
          ##
          enabled: false

          ## Annotations for ThanosRuler
          ##
          annotations: {}

          ## Service account for ThanosRuler to use.
          ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
          ##
          serviceAccount:
            create: true
            name: ""
            annotations: {}

          ## Configure pod disruption budgets for ThanosRuler
          ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
          ## This configuration is immutable once created and will require the PDB to be deleted to be changed
          ## https://github.com/kubernetes/kubernetes/issues/45398
          ##
          podDisruptionBudget:
            enabled: false
            minAvailable: 1
            maxUnavailable: ""

          ingress:
            enabled: false

            # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
            # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
            # ingressClassName: nginx

            annotations: {}

            labels: {}

            ## Hosts must be provided if Ingress is enabled.
            ##
            hosts: []
              # - thanosruler.domain.com

            ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix
            ##
            paths: []
            # - /

            ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
            ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
            # pathType: ImplementationSpecific

            ## TLS configuration for ThanosRuler Ingress
            ## Secret must be manually created in the namespace
            ##
            tls: []
            # - secretName: thanosruler-general-tls
            #   hosts:
            #   - thanosruler.example.com

          ## Configuration for ThanosRuler service
          ##
          service:
            annotations: {}
            labels: {}
            clusterIP: ""

            ## Port for ThanosRuler Service to listen on
            ##
            port: 10902
            ## To be used with a proxy extraContainer port
            ##
            targetPort: 10902
            ## Port to expose on each node
            ## Only used if service.type is 'NodePort'
            ##
            nodePort: 30905
            ## List of IP addresses at which the Prometheus server service is available
            ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
            ##

            ## Additional ports to open for ThanosRuler service
            additionalPorts: []

            externalIPs: []
            loadBalancerIP: ""
            loadBalancerSourceRanges: []

            ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
            ##
            externalTrafficPolicy: Cluster
            type: ClusterIP
          serviceMonitor:
            ## Scrape interval. If not set, the Prometheus default scrape interval is used.
            ##
            interval: ""
            selfMonitor: true

            ## proxyUrl: URL of a proxy that should be used for scraping.
            ##
            proxyUrl: ""

            ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
            scheme: ""

            ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
            ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
            tlsConfig: {}

            bearerTokenFile:

            ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            metricRelabelings: []
            # - action: keep
            #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
            #   sourceLabels: [__name__]

            ## RelabelConfigs to apply to samples before scraping
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
            ##
            relabelings: []
            # - sourceLabels: [__meta_kubernetes_pod_node_name]
            #   separator: ;
            #   regex: ^(.*)$
            #   targetLabel: nodename
            #   replacement: $1
            #   action: replace

          ## Settings affecting thanosRulerpec
          ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec
          ##
          thanosRulerSpec:
            podMetadata: {}
            image:
              repository: quay.io/thanos/thanos
              tag: v0.29.0
              sha: ""

            ## Namespaces to be selected for PrometheusRules discovery.
            ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
            ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
            ##
            ruleNamespaceSelector: {}

            ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the
            ## prometheus resource to be created with selectors based on values in the helm deployment,
            ## which will also match the PrometheusRule resources created
            ##
            ruleSelectorNilUsesHelmValues: true

            ## PrometheusRules to be selected for target discovery.
            ## If {}, select all PrometheusRules
            ##
            ruleSelector: {}
            ## Example which select all PrometheusRules resources
            ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
            # ruleSelector:
            #   matchExpressions:
            #     - key: prometheus
            #       operator: In
            #       values:
            #         - example-rules
            #         - example-rules-2
            #
            ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
            # ruleSelector:
            #   matchLabels:
            #     role: example-rules

            ## Define Log Format
            # Use logfmt (default) or json logging
            logFormat: logfmt

            ## Log level for ThanosRuler to be configured with.
            ##
            logLevel: info

            ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the
            ## running cluster equal to the expected size.
            replicas: 1

            ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression
            ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
            ##
            retention: 24h

            ## Interval between consecutive evaluations.
            ##
            evaluationInterval: ""

            ## Storage is the definition of how storage will be used by the ThanosRuler instances.
            ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
            ##
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi

            ## AlertmanagerConfig define configuration for connecting to alertmanager.
            ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.
            alertmanagersConfig: {}
            #   - api_version: v2
            #     http_config:
            #       basic_auth:
            #         username: some_user
            #         password: some_pass
            #     static_configs:
            #       - alertmanager.thanos.io
            #     scheme: http
            #     timeout: 10s

            ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.
            ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.
            # alertmanagersUrl:

            ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false
            ##
            externalPrefix:

            ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
            ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
            ##
            routePrefix: /

            ## ObjectStorageConfig configures object storage in Thanos. Alternative to
            ## ObjectStorageConfigFile, and lower order priority.
            objectStorageConfig: {}

            ## ObjectStorageConfigFile specifies the path of the object storage configuration file.
            ## When used alongside with ObjectStorageConfig, ObjectStorageConfigFile takes precedence.
            objectStorageConfigFile: ""

            ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.
            ## Maps to the --query flag of thanos ruler.
            queryEndpoints: []

            ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
            ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
            queryConfig: {}

            ## Labels configure the external label pairs to ThanosRuler. A default replica
            ## label `thanos_ruler_replica` will be always added as a label with the value
            ## of the pod's name and it will be dropped in the alerts.
            labels: {}

            ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
            ##
            paused: false

            ## Define which Nodes the Pods are scheduled on.
            ## ref: https://kubernetes.io/docs/user-guide/node-selection/
            ##
            nodeSelector:
              kubernetes.io/hostname: node-two

            ## Define resources requests and limits for single Pods.
            ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
            ##
            resources:
              limits:
                cpu: 500m
                memory: 1Gi
              requests:
                cpu: 250m
                memory: 512Mi

            ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
            ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
            ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
            ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
            ##
            podAntiAffinity: ""

            ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
            ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
            ##
            podAntiAffinityTopologyKey: kubernetes.io/hostname

            ## Assign custom affinity rules to the thanosRuler instance
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
            ##
            affinity: {}
            # nodeAffinity:
            #   requiredDuringSchedulingIgnoredDuringExecution:
            #     nodeSelectorTerms:
            #     - matchExpressions:
            #       - key: kubernetes.io/e2e-az-name
            #         operator: In
            #         values:
            #         - e2e-az1
            #         - e2e-az2

            ## If specified, the pod's tolerations.
            ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
            ##
            tolerations: []
            # - key: "key"
            #   operator: "Equal"
            #   value: "value"
            #   effect: "NoSchedule"

            ## If specified, the pod's topology spread constraints.
            ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
            ##
            topologySpreadConstraints: []
            # - maxSkew: 1
            #   topologyKey: topology.kubernetes.io/zone
            #   whenUnsatisfiable: DoNotSchedule
            #   labelSelector:
            #     matchLabels:
            #       app: thanos-ruler

            ## SecurityContext holds pod-level security attributes and common container settings.
            ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
            ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
            ##
            securityContext:
              runAsGroup: 2000
              runAsNonRoot: true
              runAsUser: 1000
              fsGroup: 2000

            ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.
            ## Note this is only for the ThanosRuler UI, not the gossip communication.
            ##
            listenLocal: false

            ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.
            ##
            containers: []

            # Additional volumes on the output StatefulSet definition.
            volumes: []

            # Additional VolumeMounts on the output StatefulSet definition.
            volumeMounts: []

            ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
            ## (permissions, dir tree) on mounted volumes before starting prometheus
            initContainers: []

            ## Priority class assigned to the Pods
            ##
            priorityClassName: ""

            ## PortName to use for ThanosRuler.
            ##
            portName: "web"

          ## ExtraSecret can be used to store various data in an extra secret
          ## (use it for example to store hashed basic auth credentials)
          extraSecret:
            ## if not set, name will be auto generated
            # name: ""
            annotations: {}
            data: {}
          #   auth: |
          #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
          #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

        ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
        ##
        cleanPrometheusOperatorObjectNames: false


  destination:
    namespace: monitoring
    name: in-cluster
  syncPolicy:
    automated:
      prune: true
      selfHeal: false
      allowEmpty: true
    syncOptions:
    - Validate=false
    - CreateNamespace=false
    - PrunePropagationPolicy=foreground
    - PruneLast=true
    - ApplyOutOfSyncOnly=false
    - Prune=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
